{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Extended_Forecasting_Tutorial_gluots-0.5",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iskra3138/GluonTS/blob/master/3_Extended_Forecasting_Tutorial_gluots_0_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1MTAECZIFKC",
        "colab_type": "text"
      },
      "source": [
        "Ref. https://gluon-ts.mxnet.io/examples/extended_forecasting_tutorial/extended_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtkVPqoH9Rfy",
        "colab_type": "text"
      },
      "source": [
        "## 환경 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CSLjjP49WWM",
        "colab_type": "text"
      },
      "source": [
        "Colab에서 어떤 GPU가 할당되었는 지 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ0D8LcKy2vu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZfXmDXu9b8y",
        "colab_type": "text"
      },
      "source": [
        "MXNET 설치 \n",
        "- GluonTS는 Amazon이 2019년 공개한 Tool로서 MXNET을 backbone으로 사용\n",
        "  - https://aws.amazon.com/blogs/machine-learning/creating-neural-time-series-models-with-gluon-time-series/\n",
        "\n",
        "- 어떤 버전의 MXNET을 설치해야 하는 지는 아래 링크 참고\n",
        "  - https://mxnet.apache.org/get_started/?platform=linux&language=python&processor=gpu&environ=pip&"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpYiCxWFwrdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nvidia-smi로 CUDA Version 확인하고 해당되는 MXNET 설치\n",
        "!pip install mxnet-cu101"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrgwGyjH-_1K",
        "colab_type": "text"
      },
      "source": [
        "GluonTS를 설치함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O45K03dixX4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gluonts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8ll-z1rIBaJ",
        "colab_type": "text"
      },
      "source": [
        "# Extended Forecasting Tutorial \n",
        "\n",
        "In the extended forecasting tutorial we cover the following topics.\n",
        "\n",
        "---\n",
        "> ## Table of contents\n",
        "> 1. ### <span style=\"color:orange\">Datasets</span>\n",
        "    1.1 Available datasets on GluonTS  \n",
        "    1.2 Create artificial datasets with GluonTS  \n",
        "    1.3 Use your time series and features\n",
        "> 2. ### <span style=\"color:orange\">Transformation</span>\n",
        "    2.1 Define a transformation  \n",
        "    2.2 Transform a dataset \n",
        "> 3. ### <span style=\"color:orange\">Training an existing model</span>\n",
        "    3.1 Configuring an estimator  \n",
        "    3.2 Getting a predictor  \n",
        "    3.3 Saving/Loading an existing model  \n",
        "> 4. ### <span style=\"color:orange\">Evaluation</span>\n",
        "    4.1 Getting the forecasts  \n",
        "    4.2 Compute metrics  \n",
        "> 5. ### <span style=\"color:orange\">Create your own model</span>\n",
        "    5.1 Point forecasts with a simple feedforward network  \n",
        "    5.2 Probabilistic forecasting  \n",
        "    5.3 Add features and scaling   \n",
        "    5.4 From feedforward to RNN\n",
        "    \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85aYYwuSIBaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Third-party imports\n",
        "%matplotlib inline\n",
        "import mxnet as mx\n",
        "from mxnet import gluon\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "from itertools import islice\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDQ1my96IBaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mx.random.seed(0)\n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGs59vHOIBaW",
        "colab_type": "text"
      },
      "source": [
        "# 1. Datasets\n",
        "\n",
        "The first requirement to use GluonTS is to have an appropriate dataset. GluonTS offers three different options to practitioners that want to experiment with the various modules: \n",
        "\n",
        "- **Use an available dataset provided by GluonTS.**\n",
        "- **Create an artificial dataset using GluonTS**\n",
        "- **Convert your dataset to a GluonTS friendly format**\n",
        "\n",
        "In general, a dataset should satisfy some minimum format requirements to be compatible with GluonTS. In particular, it should be an iterable collection of data entries (time series), and each entry should have at least **a `target` field**, which contains the actual values of the time series, and **a `start` field**, which denotes the starting date of the time series. There are many more optional fields that we will go through in this tutorial.\n",
        "\n",
        "- start field: time series가 시작하는 날짜 정보\n",
        "- target field: time series의 실제 값들\n",
        "\n",
        "The datasets provided by GluonTS come in the appropriate format and they can be used without any post processing. However, a custom dataset needs to be converted. Fortunately this is an easy task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HCTvm4oh6jx",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Available datasets on GluonTS\n",
        "\n",
        "GluonTS comes with a number of available datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9qpjNd-IBaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gluonts.dataset.repository.datasets import get_dataset, dataset_recipes\n",
        "from gluonts.dataset.util import to_pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK5baEggOK9k",
        "colab_type": "text"
      },
      "source": [
        "- version up 되면서 데이터가 더 늘어났음\n",
        "  - 'constant', 'exchange_rate', 'solar-energy', 'electricity', 'traffic', 'exchange_rate_nips', 'electricity_nips', 'traffic_nips', 'solar_nips', 'wiki-rolling_nips', 'taxi_30min', 'm4_hourly', 'm4_daily', 'm4_weekly', 'm4_monthly', 'm4_quarterly', 'm4_yearly', 'm5'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRpSd9HJIBaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Available datasets: {list(dataset_recipes.keys())}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK6L8xICIBaf",
        "colab_type": "text"
      },
      "source": [
        "To download one of the built-in datasets, simply call `get_dataset` with one of the above names. GluonTS can re-use the saved dataset so that it does not need to be downloaded again: simply set `regenerate=False`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuor5iD3IBag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = get_dataset(\"m4_hourly\", regenerate=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rI6_xUbO4ii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = get_dataset(\"m4_daily\", regenerate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNAhOCtlO4Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_daily = get_dataset(\"m4_daily\", regenerate=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vwIwRRMO83t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_daily = get_dataset(\"m4_daily\", regenerate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSW5JJmJIBak",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.1 What is in a dataset?\n",
        "\n",
        "In general, the datasets provided by GluonTS are objects that consists of three main members:\n",
        "\n",
        "- `dataset.train` is an iterable collection of data entries used for training. Each entry corresponds to one time series.\n",
        "- `dataset.test` is an iterable collection of data entries used for inference. The test dataset is an extended version of the train dataset that contains a window in the end of each time series that was not seen during training. This window has length equal to the recommended prediction length.\n",
        "- `dataset.metadata` contains metadata of the dataset such as the frequency of the time series, a recommended prediction horizon, associated features, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDfee-SJiNNN",
        "colab_type": "text"
      },
      "source": [
        "일반적으로 GluonTS에서 제공되는 Dataset들은 다음 세가지를 가지고 있음\n",
        "- dataset.train: 학습에 사용되는 iterable 데이터\n",
        "- dataset.test: 평가에 사용되는 iterable 데이터, train data보다 예측 길이만큼을 더 가지고 있음\n",
        "- dataset.metadat: 해당 데이터셋의 메타정보 포함 (time series의 frequency, 예측 길이, 관련된 feature들 등)\n",
        "\n",
        "<font color='blue'>row에 해당하는 것을 entry라고 부르는 것 같음</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEIKpGNYQKqN",
        "colab_type": "text"
      },
      "source": [
        "First, let's see what the first entry of the train dataset contains. We should expect at least a `target` and a `start` field in each entry, and the target of the test entry to have an additional window equal to `prediction_length`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDGhuhenkENq",
        "colab_type": "text"
      },
      "source": [
        "##### train data entry의 각 Field 확인 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0qNzJm3IBal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the first time series in the training set\n",
        "# training dataset의 첫번째 entry를 보겠음\n",
        "### entry에는 start field, target field, feat_statc_cat filed, source filed가 있음\n",
        "train_entry = next(iter(dataset.train))\n",
        "train_entry.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSv-EsaOIBas",
        "colab_type": "text"
      },
      "source": [
        "We observe that apart from the required fields there is one more `feat_static_cat` field (we can safely ignore the `source` field). This shows that the dataset has some features apart from the values of the time series. For now, we will ignore this field too. We will explain it in detail later with all the other optional fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwuqyvY8sOmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## train entry 개수(데이터 개수) 확인\n",
        "i = 0\n",
        "for _ in iter(dataset.train):\n",
        "  i+=1\n",
        "print (i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKwTsXCJj9Vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## Start filed 확인\n",
        "print (train_entry['start'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_99D2czDkJSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## target filed 길이 확인(time serise 길이)\n",
        "print (train_entry['target'].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTeB-4dykI_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## target field target value 앞뒤로 10개씩 확인\n",
        "print (train_entry['target'][:10])\n",
        "print (train_entry['target'][-10:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYNEvJ_8kI62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## 'feat_static_cat' field 확인\n",
        "## 'feat_static_cat' field는 일단 여기서 무시하고 넘어가고 나중에 다루겠다 함\n",
        "print (train_entry['feat_static_cat'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMDeiH48kjvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## source field 확인\n",
        "## source는 데이터가 저장되어 있는 경로에 대한 정보를 담고 있는 것 같음\n",
        "print (train_entry['source'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEP_Xdl6QokT",
        "colab_type": "text"
      },
      "source": [
        "We can similarly examine the first entry of the test dataset. We should expect exactly the same fields as in the train dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6Fq3aT5qlOLc"
      },
      "source": [
        "##### test data entry의 각 Field 확인 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZvchhPDIBas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the first time series in the test set\n",
        "test_entry = next(iter(dataset.test))\n",
        "test_entry.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cokISsuschG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## test entry 개수(데이터 개수) 확인\n",
        "### train entry 개수(414)와 같음\n",
        "i = 0\n",
        "for _ in iter(dataset.test):\n",
        "  i+=1\n",
        "print (i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A_vdpwg6lOLe",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## 'start' field 확인\n",
        "### train data, test data 동일\n",
        "print (train_entry['start'])\n",
        "print (test_entry['start'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ntsYDfeulOLi",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## target 길이 비교\n",
        "## test data가 prediction_length만큼 더 김\n",
        "print (train_entry['target'].shape)\n",
        "print (test_entry['target'].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t26XL_DilOLl",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## target value 비교\n",
        "print (train_entry['target'][:10])\n",
        "print (test_entry['target'][:10])\n",
        "print ()\n",
        "print (train_entry['target'][-10:])\n",
        "print (test_entry['target'][-58:-48])\n",
        "print (test_entry['target'][-10:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "exN-coJnlOLp",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## 'feat_Static_cat' 비교\n",
        "## 'feat_static_cat' filed는 일단 여기서 무시하고 넘어가고 나중에 다루겠다 함\n",
        "print (train_entry['feat_static_cat'])\n",
        "print (test_entry['feat_static_cat'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pGMoDw40lOLs",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## 'source' filed 비교\n",
        "## source는 데이터가 저장되어 있는 경로에 대한 정보를 담고 있는 것 같음\n",
        "print (train_entry['source'])\n",
        "print (test_entry['source'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57IeckspIBaw",
        "colab_type": "text"
      },
      "source": [
        "Moreover, we should expect that the target will have an additional window in the end with length equal to `prediction_length`. To better understand what this means we can visualize both the train and test time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzWr1ZDIIBax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_series = to_pandas(test_entry)\n",
        "train_series = to_pandas(train_entry)\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(10, 7))\n",
        "\n",
        "train_series.plot(ax=ax[0])\n",
        "ax[0].grid(which=\"both\")\n",
        "ax[0].legend([\"train series\"], loc=\"upper left\")\n",
        "\n",
        "test_series.plot(ax=ax[1])\n",
        "ax[1].axvline(train_series.index[-1], color='r') # end of train dataset\n",
        "ax[1].grid(which=\"both\")\n",
        "ax[1].legend([\"test series\", \"end of train series\"], loc=\"upper left\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eRKIf9fqn2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_series"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DNK3ljlqdoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_series"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNUImMCvIBa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Length of forecasting window in test dataset: {len(test_series) - len(train_series)}\")\n",
        "print(f\"Recommended prediction horizon: {dataset.metadata.prediction_length}\")\n",
        "print(f\"Frequency of the time series: {dataset.metadata.freq}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkfjkBqKIBa5",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Create artificial datasets\n",
        "\n",
        "We can easily create a complex artificial time series dataset using the `ComplexSeasonalTimeSeries` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUplbHzyIBa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 0.5에서 불러오는 곳이 바뀜\n",
        "#from gluonts.dataset.artificial._base import ComplexSeasonalTimeSeries\n",
        "from gluonts.dataset.artificial import ComplexSeasonalTimeSeries\n",
        "from gluonts.dataset.common import ListDataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEXWnOcUIBa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "artificial_dataset = ComplexSeasonalTimeSeries(\n",
        "    num_series=10, #number of time series generated in the train and test set\n",
        "    prediction_length=21,\n",
        "    freq_str=\"H\",\n",
        "    length_low=30, #minimum length of a time-series, must be larger than prediction_length\n",
        "    length_high=200, #maximum length of a time-series\n",
        "    min_val=-10000, #min value of a time-series\n",
        "    max_val=10000, #max value of a time-series\n",
        "    is_integer=False, #whether the dataset has integers or not\n",
        "    proportion_missing_values=0,\n",
        "    is_noise=True, #whether to add noise\n",
        "    is_scale=True, #whether to add scale\n",
        "    percentage_unique_timestamps=1, #percentage of random start dates bounded between 0 and 1\n",
        "    is_out_of_bounds_date=True, #determines whether to use very old start dates and start dates far in the future\n",
        ")\n",
        "\n",
        "#param seasonality: Seasonality of the generated data. If not given uses default seasonality for frequency\n",
        "#param clip_values: if True the values will be clipped to [min_val, max_val], otherwise linearly scales them"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoGeyZRNIBbE",
        "colab_type": "text"
      },
      "source": [
        "We can access some important metadata of the artificial dataset as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLZCKDo3IBbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"prediction length: {artificial_dataset.metadata.prediction_length}\")\n",
        "print(f\"frequency: {artificial_dataset.metadata.freq}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB5BRQ6uIBbI",
        "colab_type": "text"
      },
      "source": [
        "The artificial dataset that we created is a list of dictionaries. Each dictionary corresponds to a time series and it should contain the required fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxC_CGVzIBbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"type of train dataset: {type(artificial_dataset.train)}\")\n",
        "print(f\"train dataset fields: {artificial_dataset.train[0].keys()}\")\n",
        "print(f\"type of test dataset: {type(artificial_dataset.test)}\")\n",
        "print(f\"test dataset fields: {artificial_dataset.test[0].keys()}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OV9YJhltWdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# 10개의 entry가 만들어졌음을 볼 수 있음\n",
        "artificial_dataset.train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA_54toqu4ju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# 10개의 entry가 만들어졌음을 볼 수 있음\n",
        "artificial_dataset.test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nphuU7-hvCxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# train과 test가 같은 start임을 볼 수 있음\n",
        "for i in range(10):\n",
        "  print (artificial_dataset.train[i]['start'], 'vs.', artificial_dataset.test[i]['start'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oQhG_XUvY6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# target lentgh를 보면 30~200 사이의 값이고 test가 prediction_length인 21만큼 더 큼을 볼 수 있음\n",
        "for i in range(10):\n",
        "  print (artificial_dataset.train[i]['target'][:2], 'vs.', artificial_dataset.test[i]['target'][:2], end='\\t')\n",
        "  print (artificial_dataset.train[i]['target'].shape, 'vs.', artificial_dataset.test[i]['target'].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhxaO8uYv6da",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# target value도 -10000 ~ 10000 사이임을 볼 수 있음\n",
        "for i in range(10):\n",
        "  print (min(artificial_dataset.train[i]['target']) , 'vs.', max(artificial_dataset.test[i]['target']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLSkWVeJYIHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# percentage_unique_timestamps 변경 실험\n",
        "artificial_dataset2 = ComplexSeasonalTimeSeries(\n",
        "    num_series=10, #number of time series generated in the train and test set\n",
        "    prediction_length=21,\n",
        "    freq_str=\"H\",\n",
        "    length_low=30, #minimum length of a time-series, must be larger than prediction_length\n",
        "    length_high=200, #maximum length of a time-series\n",
        "    min_val=-10000, #min value of a time-series\n",
        "    max_val=10000, #max value of a time-series\n",
        "    is_integer=False, #whether the dataset has integers or not\n",
        "    proportion_missing_values=0,\n",
        "    is_noise=True, #whether to add noise\n",
        "    is_scale=True, #whether to add scale\n",
        "    percentage_unique_timestamps=0, #percentage of random start dates bounded between 0 and 1\n",
        "    is_out_of_bounds_date=True, #determines whether to use very old start dates and start dates far in the future\n",
        ")\n",
        "\n",
        "#param seasonality: Seasonality of the generated data. If not given uses default seasonality for frequency\n",
        "#param clip_values: if True the values will be clipped to [min_val, max_val], otherwise linearly scales them\n",
        "\n",
        "for i in range(10):\n",
        "  print (artificial_dataset2.train[i]['start'], 'vs.', artificial_dataset2.test[i]['start'])\n",
        "\n",
        "## is_out_of_bounds_date로 인해 아주 먼 과거와 아주 먼 미래 데이터가 추가됨을 볼 수 있음"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOGJU7qtYICb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# percentage_unique_timestamps 변경 및 is_out_of_bounds_date 변경 실험\n",
        "artificial_dataset2 = ComplexSeasonalTimeSeries(\n",
        "    num_series=10, #number of time series generated in the train and test set\n",
        "    prediction_length=21,\n",
        "    freq_str=\"H\",\n",
        "    length_low=30, #minimum length of a time-series, must be larger than prediction_length\n",
        "    length_high=200, #maximum length of a time-series\n",
        "    min_val=-10000, #min value of a time-series\n",
        "    max_val=10000, #max value of a time-series\n",
        "    is_integer=False, #whether the dataset has integers or not\n",
        "    proportion_missing_values=0,\n",
        "    is_noise=True, #whether to add noise\n",
        "    is_scale=True, #whether to add scale\n",
        "    percentage_unique_timestamps=0, #percentage of random start dates bounded between 0 and 1\n",
        "    is_out_of_bounds_date=False, #determines whether to use very old start dates and start dates far in the future\n",
        ")\n",
        "\n",
        "#param seasonality: Seasonality of the generated data. If not given uses default seasonality for frequency\n",
        "#param clip_values: if True the values will be clipped to [min_val, max_val], otherwise linearly scales them\n",
        "\n",
        "for i in range(10):\n",
        "  print (artificial_dataset2.train[i]['start'], 'vs.', artificial_dataset2.test[i]['start'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaw8gjcJYH95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvmT4Jk3IBbM",
        "colab_type": "text"
      },
      "source": [
        "**In order to use the artificially created datasets (list of dictionaries) we need to convert them to `ListDataset` objects.**\n",
        "- ListDataset을 통해 generator를 만드는 것 같음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEVDe11qIBbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = ListDataset(artificial_dataset.train, \n",
        "                        freq=artificial_dataset.metadata.freq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQr-PP8mIBbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_ds = ListDataset(artificial_dataset.test, \n",
        "                       freq=artificial_dataset.metadata.freq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORc4RxZeIBbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_entry = next(iter(train_ds))\n",
        "train_entry.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0mckG6OIBbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_entry = next(iter(test_ds))\n",
        "test_entry.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N1yjFJhIBba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_series = to_pandas(test_entry)\n",
        "train_series = to_pandas(train_entry)\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(10, 7))\n",
        "\n",
        "train_series.plot(ax=ax[0])\n",
        "ax[0].grid(which=\"both\")\n",
        "ax[0].legend([\"train series\"], loc=\"upper left\")\n",
        "\n",
        "test_series.plot(ax=ax[1])\n",
        "ax[1].axvline(train_series.index[-1], color='r') # end of train dataset\n",
        "ax[1].grid(which=\"both\")\n",
        "ax[1].legend([\"test series\", \"end of train series\"], loc=\"upper left\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0IrskPQIBbe",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Use your time series and features\n",
        "\n",
        "Now, we will see how we can convert any custom dataset with any associated features to an appropriate format for GluonTS.\n",
        "\n",
        "As already mentioned a dataset is required to have at least the `target` and the `start` fields. However, it may have more. Let's see what are all the available fields:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jelxTl5OIBbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gluonts.dataset.field_names import FieldName"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lgprN1mIBbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[f\"FieldName.{k} = '{v}'\" for k, v in FieldName.__dict__.items() if not k.startswith('_')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gzPCe3OIBbk",
        "colab_type": "text"
      },
      "source": [
        "The fields are split into three categories: the required ones, the optional ones, and the ones that can be added by the `Transformation` (explained in a while).\n",
        "\n",
        "Required:\n",
        "\n",
        "- `start`: start date of the time series\n",
        "- `target`: values of the time series\n",
        "\n",
        "Optional:\n",
        "\n",
        "- `feat_static_cat`: static (over time) categorical features, list with dimension equal to the number of features (시계열 데이터 개수와 동일)\n",
        "- `feat_static_real`: static (over time) real features, list with dimension equal to the number of features\n",
        "- `feat_dynamic_cat`: dynamic (over time) categorical features, array with shape equal to (number of features, target length)\n",
        "- `feat_dynamic_real`: dynamic (over time) real features, array with shape equal to (number of features, target length)\n",
        "\n",
        "Added by `Transformation`:\n",
        "\n",
        "- `time_feat`: time related features such as the month or the day \n",
        "- `feat_dynamic_const`: expands a constant value feature along the time axis\n",
        "- `feat_dynamic_age`: age feature, i.e., a feature that its value is small for distant past timestamps and it monotonically increases the more we approach the current timestamp\n",
        "- `observed_values`: indicator for observed values, i.e., a feature that equals to 1 if the value is observed and 0 if the value is missing\n",
        "- `is_pad`: indicator for each time step that shows if it is padded (if the length is not enough) \n",
        "- `forecast_start`: forecast start date\n",
        "\n",
        "As a simple example, we can create a custom dataset to see how we can use some of these fields. The dataset consists of <font color=blue> **a target, a real dynamic feature** </font> (which in this example we set to be the target value one period earlier), and <font color=blue>**a static categorical feature**</font> that indicates the sinusoid type (different phase) that we used to create the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBFcggggIBbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(num_series, num_steps, period=24, mu=1, sigma=0.3):\n",
        "    # create target: noise + pattern    \n",
        "    # noise\n",
        "    noise = np.random.normal(mu, sigma, size=(num_series, num_steps))\n",
        "    \n",
        "    # pattern - sinusoid with different phase\n",
        "    sin_minumPi_Pi = np.sin(np.tile(np.linspace(-np.pi, np.pi, period), int(num_steps / period)))\n",
        "    sin_Zero_2Pi = np.sin(np.tile(np.linspace(0, 2 * np.pi, 24), int(num_steps / period)))\n",
        "    \n",
        "    pattern = np.concatenate((np.tile(sin_minumPi_Pi.reshape(1, -1), \n",
        "                                      (int(np.ceil(num_series / 2)),1)), \n",
        "                              np.tile(sin_Zero_2Pi.reshape(1, -1), \n",
        "                                      (int(np.floor(num_series / 2)), 1))\n",
        "                             ),\n",
        "                             axis=0\n",
        "                            )\n",
        "    \n",
        "    target = noise + pattern\n",
        "    \n",
        "    # create time features: use target one period earlier, append with zeros\n",
        "    ## 한 주기 전의 값을 dynamic 실수형 피쳐로 사용\n",
        "    feat_dynamic_real = np.concatenate((np.zeros((num_series, period)), \n",
        "                                        target[:, :-period]\n",
        "                                       ), \n",
        "                                       axis=1\n",
        "                                      )\n",
        "    \n",
        "    # create categorical static feats: use the sinusoid type as a categorical feature\n",
        "    ## 사인형태를 썼는지, 아닌 지를 나타내는 static 범주형 피쳐 사용\n",
        "    feat_static_cat = np.concatenate((np.zeros(int(np.ceil(num_series / 2))), \n",
        "                                      np.ones(int(np.floor(num_series / 2)))\n",
        "                                     ),\n",
        "                                     axis=0\n",
        "                                    )\n",
        "    \n",
        "    return target, feat_dynamic_real, feat_static_cat\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF1wq8UZWi0w",
        "colab_type": "text"
      },
      "source": [
        "##### create_dataset 함수 분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLZ7Y_fvRf7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# noise 확인\n",
        "mu, sigma, num_series, num_steps = 1, 0.3, 100, 24*7\n",
        "noise = np.random.normal(mu, sigma, size=(num_series, num_steps))\n",
        "print (noise.shape)\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "plt.plot(noise[0])\n",
        "plt.grid(which=\"both\")\n",
        "plt.legend([\"noise\"], loc=\"upper left\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEpiWrxCRf2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# pattern1 확인 :sinusoid with different phase\n",
        "period, num_steps = 24, 24*7\n",
        "sin_minumPi_Pi = np.sin(np.tile(np.linspace(-np.pi, np.pi, period), int(num_steps / period)))\n",
        "\n",
        "print (sin_minumPi_Pi.shape)\n",
        "print (sin_minumPi_Pi[23:25])\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "plt.plot(sin_minumPi_Pi)\n",
        "plt.grid(which=\"both\")\n",
        "plt.legend([\"sin_minumPi_Pi\"], loc=\"upper left\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9qSxcnpRfx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# pattern2 확인 :sinusoid with different phase\n",
        "period, num_steps = 24, 24*7\n",
        "\n",
        "sin_Zero_2Pi = np.sin(np.tile(np.linspace(0, 2 * np.pi, 24), int(num_steps / period)))\n",
        "\n",
        "print (sin_Zero_2Pi.shape)\n",
        "print (sin_Zero_2Pi[23:25])\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "plt.plot(sin_Zero_2Pi)\n",
        "plt.grid(which=\"both\")\n",
        "plt.legend([\"sin_Zero_2Pi\"], loc=\"upper left\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VzlYstSRfoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# pattern code 분석 --> sin_minumPi_Pi를 앞에 50개, sin_Zero_2Pi를 뒤에 50개 concat해서 (100, 168) shape의 array 생성\n",
        "pattern = np.concatenate((np.tile(sin_minumPi_Pi.reshape(1, -1), \n",
        "                                      (int(np.ceil(num_series / 2)),1)), \n",
        "                              np.tile(sin_Zero_2Pi.reshape(1, -1), \n",
        "                                      (int(np.floor(num_series / 2)), 1))\n",
        "                             ),\n",
        "                             axis=0\n",
        "                            )\n",
        "print (pattern.shape)\n",
        "## 상세는 아래 코드셀 3개 참조"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbX2NTHzWtL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# np.tile(sin_minumPi_Pi.reshape(1, -1), (int(np.ceil(num_series / 2)),1)) --> sin_minumPi_Pi를 50개 반복\n",
        "print (sin_minumPi_Pi.shape, '->' , sin_minumPi_Pi.reshape(1, -1).shape)\n",
        "print (int(np.ceil(num_series / 2)))\n",
        "\n",
        "pattern1 = np.tile(sin_minumPi_Pi.reshape(1, -1), (int(np.ceil(num_series / 2)),1))\n",
        "print (pattern1.shape)\n",
        "print (pattern1[0][:3], pattern1[-1][:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQfSXlkjWtI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# np.tile(sin_Zero_2Pi.reshape(1, -1), (int(np.floor(num_series / 2)), 1)) --> sin_Zero_2Pi를 50개 반복\n",
        "print (sin_minumPi_Pi.shape, '->' , sin_minumPi_Pi.reshape(1, -1).shape)\n",
        "print (int(np.ceil(num_series / 2)))\n",
        "\n",
        "pattern2 = np.tile(sin_Zero_2Pi.reshape(1, -1), (int(np.floor(num_series / 2)), 1))\n",
        "print (pattern2.shape)\n",
        "print (pattern2[0][:3], pattern2[-1][:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0elc9YwY61_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# 168개의 value를 갖는 100개의 element들, 즉 (100, 168) shape의 noise와 pattern을 더해서 target 데이터 생성\n",
        "target = noise + pattern\n",
        "print (noise.shape, pattern.shape, target.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knxoDjL4Y6zS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# create time features: use target one period earlier, append with zeros\n",
        "# 이전 period 값을 feat_dynamic_real로 사용한다고 가정하였으므로, 첫번째 period는 전부 0으로, 나머지 period는 하나 전 period 값으로 대체\n",
        "feat_dynamic_real = np.concatenate((np.zeros((num_series, period)), \n",
        "                                    target[:, :-period]\n",
        "                                    ), \n",
        "                                    axis=1\n",
        "                                  )\n",
        "print (feat_dynamic_real.shape)\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(20, 7))\n",
        "\n",
        "ax[0].plot(target[0])\n",
        "ax[0].grid(which=\"both\")\n",
        "ax[0].legend([\"target\"], loc=\"upper left\")\n",
        "\n",
        "ax[1].plot(feat_dynamic_real[0])\n",
        "ax[1].axvline(period, color='r') # end of train dataset\n",
        "ax[1].grid(which=\"both\")\n",
        "ax[1].legend([\"feat_dynamic_real\", \"end of train series\"], loc=\"upper left\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-RgCwt1Y6wL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# create categorical static feats: use the sinusoid type as a categorical feature\n",
        "## sin_minumPi_Pi가 사용된 앞에 50개는 0, sin_Zero_2Pi가 사용된 뒤에 50개는 1이 됨\n",
        "feat_static_cat = np.concatenate((np.zeros(int(np.ceil(num_series / 2))), \n",
        "                                  np.ones(int(np.floor(num_series / 2)))\n",
        "                                  ),\n",
        "                                  axis=0\n",
        "                                )\n",
        "\n",
        "print (feat_static_cat.shape)\n",
        "print (feat_static_cat[:50])\n",
        "print (feat_static_cat[50:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWicJyDabYTp",
        "colab_type": "text"
      },
      "source": [
        "##### original code 복귀"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmKrutIVIBbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the parameters of the dataset\n",
        "custom_ds_metadata = {'num_series': 100,\n",
        "                      'num_steps': 24 * 7,\n",
        "                      'prediction_length': 24,\n",
        "                      'freq': '1H',\n",
        "                      'start': [pd.Timestamp(\"01-01-2019\", freq='1H') \n",
        "                                for _ in range(100)]\n",
        "                     }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjgiDO9aRHRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# start 확인\n",
        "## 100개의 series 모두 같은 시간대(01-01-2019)에서 같은 주기(1H)로 시작\n",
        "[pd.Timestamp(\"01-01-2019\", freq='1H') for _ in range(100)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgB5IqwZIBbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_out = create_dataset(custom_ds_metadata['num_series'], \n",
        "                          custom_ds_metadata['num_steps'],                                                      \n",
        "                          custom_ds_metadata['prediction_length']\n",
        "                         )\n",
        "\n",
        "target, feat_dynamic_real, feat_static_cat = data_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbF7kRtRIBbu",
        "colab_type": "text"
      },
      "source": [
        "We can easily create the train and test datasets by simply filling in the correct fields. Remember that for the train dataset we need to cut the last window.\n",
        "- artificial data에서 보았던 ListDataset을 또 활용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xZPHPOlIBbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''train_ds = ListDataset([{FieldName.TARGET: target, \n",
        "                         FieldName.START: start,\n",
        "                         FieldName.FEAT_DYNAMIC_REAL: fdr,\n",
        "                         FieldName.FEAT_STATIC_CAT: fsc} \n",
        "                        for (target, start, fdr, fsc) in zip(target[:, :-custom_ds_metadata['prediction_length']], \n",
        "                                                             custom_ds_metadata['start'], \n",
        "                                                             feat_dynamic_real[:, :-custom_ds_metadata['prediction_length']], \n",
        "                                                             feat_static_cat)],\n",
        "                      freq=custom_ds_metadata['freq'])'''\n",
        "# 0.5에서 문법이 바뀜\n",
        "train_ds = ListDataset([{FieldName.TARGET: target,\n",
        "                         FieldName.START: start,\n",
        "                         FieldName.FEAT_DYNAMIC_REAL: [fdr],\n",
        "                         FieldName.FEAT_STATIC_CAT: [fsc]}\n",
        "                        for (target, start, fdr, fsc) in zip(target[:, :-custom_ds_metadata['prediction_length']],\n",
        "                                                             custom_ds_metadata['start'],\n",
        "                                                             feat_dynamic_real[:, :-custom_ds_metadata['prediction_length']],\n",
        "                                                             feat_static_cat)],\n",
        "                      freq=custom_ds_metadata['freq'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JdBtcQXcXZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# train_dataset 확인\n",
        "tmp = [{FieldName.TARGET: target,\n",
        "                         FieldName.START: start,\n",
        "                         FieldName.FEAT_DYNAMIC_REAL: [fdr],\n",
        "                         FieldName.FEAT_STATIC_CAT: [fsc]}\n",
        "                        for (target, start, fdr, fsc) in zip(target[:, :-custom_ds_metadata['prediction_length']],\n",
        "                                                             custom_ds_metadata['start'],\n",
        "                                                             feat_dynamic_real[:, :-custom_ds_metadata['prediction_length']],\n",
        "                                                             feat_static_cat)]\n",
        "print (tmp[0]['target'].shape) # 168 - 24 = 144\n",
        "print (tmp[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1GbNjTWIBbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''test_ds = ListDataset([{FieldName.TARGET: target, \n",
        "                        FieldName.START: start,\n",
        "                        FieldName.FEAT_DYNAMIC_REAL: fdr,\n",
        "                        FieldName.FEAT_STATIC_CAT: fsc} \n",
        "                       for (target, start, fdr, fsc) in zip(target, \n",
        "                                                            custom_ds_metadata['start'], \n",
        "                                                            feat_dynamic_real, \n",
        "                                                            feat_static_cat)],\n",
        "                     freq=custom_ds_metadata['freq'])'''\n",
        "# 0.5에서 문법 바뀜\n",
        "test_ds = ListDataset([{FieldName.TARGET: target,\n",
        "                        FieldName.START: start,\n",
        "                        FieldName.FEAT_DYNAMIC_REAL: [fdr],\n",
        "                        FieldName.FEAT_STATIC_CAT: [fsc]}\n",
        "                       for (target, start, fdr, fsc) in zip(target,\n",
        "                                                            custom_ds_metadata['start'],\n",
        "                                                            feat_dynamic_real,\n",
        "                                                            feat_static_cat)],\n",
        "                     freq=custom_ds_metadata['freq'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cgVrkZAeF7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "tmp = [{FieldName.TARGET: target,\n",
        "                        FieldName.START: start,\n",
        "                        FieldName.FEAT_DYNAMIC_REAL: [fdr],\n",
        "                        FieldName.FEAT_STATIC_CAT: [fsc]}\n",
        "                       for (target, start, fdr, fsc) in zip(target,\n",
        "                                                            custom_ds_metadata['start'],\n",
        "                                                            feat_dynamic_real,\n",
        "                                                            feat_static_cat)]\n",
        "print (tmp[0]['target'].shape) # 168\n",
        "print (tmp[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RIYK7lEIBb1",
        "colab_type": "text"
      },
      "source": [
        "Now, we can examine each **entry** of the train and test datasets. We should expect that they have the following fields: `target`, `start`, `feat_dynamic_real` and `feat_static_cat`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stnjfGDkIBb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_entry = next(iter(train_ds))\n",
        "train_entry.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwKQv-BLIBb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_entry = next(iter(test_ds))\n",
        "test_entry.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtZcTpiBeb9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# source 확인\n",
        "print (train_entry['source'])\n",
        "print (test_entry['source'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7d6xFI2ev0V",
        "colab_type": "text"
      },
      "source": [
        "iter를 따로 빼면 source의 row가 바뀌는 것을 볼 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xspz-SQeemfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "test_iter = iter(test_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOm0xlJWekof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "for i in range(5):\n",
        "  test_entry_sepa = next(test_iter)\n",
        "  print (test_entry_sepa['source'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJJdknGye9Hc",
        "colab_type": "text"
      },
      "source": [
        "original code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHr6RcNFpI5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_series = to_pandas(test_entry)\n",
        "train_series = to_pandas(train_entry)\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(10, 7))\n",
        "\n",
        "train_series.plot(ax=ax[0])\n",
        "ax[0].grid(which=\"both\")\n",
        "ax[0].legend([\"train series\"], loc=\"upper left\")\n",
        "\n",
        "test_series.plot(ax=ax[1])\n",
        "ax[1].axvline(train_series.index[-1], color='r') # end of train dataset\n",
        "ax[1].grid(which=\"both\")\n",
        "ax[1].legend([\"test series\", \"end of train series\"], loc=\"upper left\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igK1ROSNIBb_",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>*For the rest of the tutorial we will use the custom dataset*</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywk55LwQIBcA",
        "colab_type": "text"
      },
      "source": [
        "# 2. Transformation\n",
        "\n",
        "## 2.1 Define a transformation\n",
        "\n",
        "The primary use case for a `Transformation` is for **feature processing**, e.g., adding a holiday **feature** and for defining the way the dataset will be split into appropriate windows during training and inference. \n",
        "\n",
        "In general, it gets an iterable collection of **entries** of a dataset and transform it to another iterable collection that can possibly contain more fields. The transformation is done by defining a set of \"actions\" to the raw dataset depending on what is useful to our model. This actions usually create some additional **features** or transform an existing **feature**. As an example, in the following we add the following transformations:\n",
        "\n",
        "- `AddObservedValuesIndicator`: Creates the `observed_values` field in the dataset, i.e., adds a **feature** that equals to 1 if the value is observed and 0 if the value is missing \n",
        "- `AddAgeFeature`: Creates the `feat_dynamic_age` field in the dataset, i.e., adds a **feature** that its value is small for distant past timestamps and it monotonically increases the more we approach the current timestamp   \n",
        "\n",
        "The `Transformation` may not define an additional field in the dataset. However, it **always** needs to define how the datasets are going to be split in example windows during training and testing. This is done with the `InstanceSplitter` that is configured as follows (skipping the obvious fields):\n",
        "\n",
        "- `is_pad_field`: indicator if the time series is padded (if the length is not enough)\n",
        "- `train_sampler`: defines how the training windows are cut/sampled\n",
        "- `time_series_fields`: contains the time dependent **features** that need to be split in the same manner as the target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbRop1EFIBcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gluonts.transform import (\n",
        "    AddAgeFeature,\n",
        "    AddObservedValuesIndicator,\n",
        "    Chain,\n",
        "    ExpectedNumInstanceSampler,\n",
        "    InstanceSplitter,\n",
        "    SetFieldIfNotPresent,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2IHX_r-IBcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_transformation(freq, context_length, prediction_length):\n",
        "    return Chain(\n",
        "        [\n",
        "            AddObservedValuesIndicator(\n",
        "                target_field=FieldName.TARGET,\n",
        "                output_field=FieldName.OBSERVED_VALUES,\n",
        "            ),\n",
        "            AddAgeFeature(\n",
        "                target_field=FieldName.TARGET,\n",
        "                output_field=FieldName.FEAT_AGE,\n",
        "                pred_length=prediction_length,\n",
        "                log_scale=True,\n",
        "            ),\n",
        "            InstanceSplitter(\n",
        "                target_field=FieldName.TARGET,\n",
        "                is_pad_field=FieldName.IS_PAD, ## indicator if the time series is padded (if the length is not enough)\n",
        "                start_field=FieldName.START,\n",
        "                forecast_start_field=FieldName.FORECAST_START,\n",
        "                train_sampler=ExpectedNumInstanceSampler(num_instances=1), ## defines how the training windows are cut/sampled\n",
        "                past_length=context_length,\n",
        "                future_length=prediction_length,\n",
        "                time_series_fields=[  ## contains the time dependent features that need to be split in the same manner as the target\n",
        "                    FieldName.FEAT_AGE,\n",
        "                    FieldName.FEAT_DYNAMIC_REAL,\n",
        "                    FieldName.OBSERVED_VALUES,\n",
        "                ],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa3JC6HVIBcG",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Transform a dataset\n",
        "\n",
        "Now, we can create a transformation object by applying the above transformation to the custom dataset we have created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRGxIWZFIBcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformation = create_transformation(custom_ds_metadata['freq'], \n",
        "                                       2 * custom_ds_metadata['prediction_length'], # can be any appropriate value\n",
        "                                       custom_ds_metadata['prediction_length'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxzOIHvDIBcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tf = transformation(iter(train_ds), is_train=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIkwmj_RIBcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(train_tf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arwksIAyIBcO",
        "colab_type": "text"
      },
      "source": [
        "As expected, the output is another iterable object. We can easily examine what is contained in an entry of the transformed dataset. When `is_train=True` in the transformation, the `InstanceSplitter` iterates over the transformed dataset and cuts windows by selecting randomly a time series and a starting point on that time series (this \"randomness\" is defined by the `train_sampler`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew1fPSu3IBcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tf_entry = next(iter(train_tf))\n",
        "[k for k in train_tf_entry.keys()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrMfKEJE1QQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## time_series_fields에 정의된 FEAT_AGE, FEAT_DYNAMIC_REAL, OBSERVED_VALUES와 target값이 past/future로 나뉘어졌음을 볼 수 있음\n",
        "print ('start:', train_tf_entry['start'])\n",
        "print ('feat_static_cat:', train_tf_entry['feat_static_cat'].shape)\n",
        "print ('source:', train_tf_entry['source'])\n",
        "print ('past_feat_dynamic_age:', train_tf_entry['past_feat_dynamic_age'].shape)\n",
        "print ('future_feat_dynamic_age:', train_tf_entry['future_feat_dynamic_age'].shape)\n",
        "print ('past_feat_dynamic_real:', train_tf_entry['past_feat_dynamic_real'].shape)\n",
        "print ('future_feat_dynamic_real:', train_tf_entry['future_feat_dynamic_real'].shape)\n",
        "print ('past_observed_values:', train_tf_entry['past_observed_values'].shape)\n",
        "print ('future_observed_values:', train_tf_entry['future_observed_values'].shape)\n",
        "print ('past_target:', train_tf_entry['past_target'].shape)\n",
        "print ('future_target:', train_tf_entry['future_target'].shape)\n",
        "print ('past_is_pad:', train_tf_entry['past_is_pad'].shape)\n",
        "print ('forecast_start:', train_tf_entry['forecast_start'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QaswgWIwC7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "print ('start:', train_tf_entry['start'])\n",
        "print ('feat_static_cat:', train_tf_entry['feat_static_cat'])\n",
        "print ('source:', train_tf_entry['source'])\n",
        "print ('past_feat_dynamic_age:', train_tf_entry['past_feat_dynamic_age'])\n",
        "print ('future_feat_dynamic_age:', train_tf_entry['future_feat_dynamic_age'])\n",
        "print ('past_feat_dynamic_real:', train_tf_entry['past_feat_dynamic_real'])\n",
        "print ('future_feat_dynamic_real:', train_tf_entry['future_feat_dynamic_real'])\n",
        "print ('past_observed_values:', train_tf_entry['past_observed_values'])\n",
        "print ('future_observed_values:', train_tf_entry['future_observed_values'])\n",
        "print ('past_target:', train_tf_entry['past_target'])\n",
        "print ('future_target:', train_tf_entry['future_target'])\n",
        "print ('past_is_pad:', train_tf_entry['past_is_pad'])\n",
        "print ('forecast_start:', train_tf_entry['forecast_start'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOr9nrxK1wAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## source가 row=2이고\n",
        "## start: 2019-01-01 00:00:00,  forecast_start: 2019-01-03 06:00:00 이며, (54시간 차이)\n",
        "## context_length가 48이므로,\n",
        "print (train_tf_entry['past_target'])\n",
        "print (target[2][54-48:54])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG4FTmbH2Umk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## source가 row=2이고\n",
        "## start: 2019-01-01 00:00:00,  forecast_start: 2019-01-03 06:00:00 이며, (54시간 차이)\n",
        "## prediction_length가 24이므로,\n",
        "print (train_tf_entry['future_target'])\n",
        "print (target[2][54:54+24])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-1mxpFvuSSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "train_tf = transformation(iter(train_ds), is_train=True)\n",
        "#train_tf = transformation(iter(test_ds), is_train=True)\n",
        "train_gen = iter(train_tf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFLQ2Ra7tOKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## randomly sampled results 확인\n",
        "\n",
        "for i in range(110) :\n",
        "  train_tmp_entry = next(train_gen)\n",
        "  #print (train_tmp_entry['start'])\n",
        "  print (i, train_tmp_entry['source'], \n",
        "         train_tmp_entry['forecast_start'] , \n",
        "         (train_tmp_entry['forecast_start'] - train_tmp_entry['start'] ) \n",
        "         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI3Bmi2hIBcT",
        "colab_type": "text"
      },
      "source": [
        "The transformer has done what we asked. In particular it has added:\n",
        "\n",
        "- a field for observed values (`observed_values`)  \n",
        "- a field for the age feature (`feat_dynamic_age`)\n",
        "- some extra useful fields (`past_is_pad`, `forecast_start`)\n",
        "\n",
        "It has done one more important thing: it has split the window into past and future and has added the corresponding prefixes to all time dependent fields. This way we can easily use e.g., the `past_target` field as input and the `future_target` field to calculate the error of our predictions. Of course, the length of the past is equal to the `context_length` and of the future equal to the `prediction_length`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUSMxRvOIBcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"past target shape: {train_tf_entry['past_target'].shape}\")\n",
        "print(f\"future target shape: {train_tf_entry['future_target'].shape}\")\n",
        "print(f\"past observed values shape: {train_tf_entry['past_observed_values'].shape}\")\n",
        "print(f\"future observed values shape: {train_tf_entry['future_observed_values'].shape}\")\n",
        "print(f\"past age feature shape: {train_tf_entry['past_feat_dynamic_age'].shape}\")\n",
        "print(f\"future age feature shape: {train_tf_entry['future_feat_dynamic_age'].shape}\")\n",
        "print(train_tf_entry['feat_static_cat'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAljo24EIBcW",
        "colab_type": "text"
      },
      "source": [
        "Just for comparison, let's see again what were the fields in the original dataset before the transformation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty1c6DzpIBcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[k for k in next(iter(train_ds)).keys()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nasShZDIBca",
        "colab_type": "text"
      },
      "source": [
        "Now, we can move on and see how the test dataset is split. As we saw, the transformation splits the windows into past and future. However, during inference (`is_train=False` in the transformation), the splitter always cuts the last window (of length `context_length`) of the dataset so it can be used to predict the subsequent unknown values of length `prediction_length`. \n",
        "\n",
        "So, how is the test dataset split in past and future since we do not know the future target? And what about the time dependent features?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCNhvxr-IBcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_tf = transformation(iter(test_ds), is_train=False)\n",
        "#test_tf = transformation(iter(train_ds), is_train=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUNEhBDDIBce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_tf_entry = next(iter(test_tf))\n",
        "[k for k in test_tf_entry.keys()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UZ5gCHOIBch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"past target shape: {test_tf_entry['past_target'].shape}\")\n",
        "print(f\"future target shape: {test_tf_entry['future_target'].shape}\")\n",
        "print(f\"past observed values shape: {test_tf_entry['past_observed_values'].shape}\")\n",
        "print(f\"future observed values shape: {test_tf_entry['future_observed_values'].shape}\")\n",
        "print(f\"past age feature shape: {test_tf_entry['past_feat_dynamic_age'].shape}\")\n",
        "print(f\"future age feature shape: {test_tf_entry['future_feat_dynamic_age'].shape}\")\n",
        "print(test_tf_entry['feat_static_cat'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJJZ0qOgIBcm",
        "colab_type": "text"
      },
      "source": [
        "The future target is empty but not the features - we always assume that we know the future features!\n",
        "\n",
        "-> 하지만, `future_feat_dynamic_real`은 (0,1)임\n",
        "\n",
        "All the things we did manually here are done by an internal block called `DataLoader`. It gets as an input the raw dataset (in appropriate format) and the transformation object and it outputs the transformed iterable dataset batch by batch. The only thing that we need to worry about is setting the transformation fields correctly!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdyLrYRX5O5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HfDDtBbP5Wi9",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## time_series_fields에 정의된 FEAT_AGE, FEAT_DYNAMIC_REAL, OBSERVED_VALUES와 target값이 past/future로 나뉘어졌음을 볼 수 있음\n",
        "'''\n",
        "['start',\n",
        " 'feat_static_cat',\n",
        " 'source',\n",
        " 'past_feat_dynamic_age',\n",
        " 'future_feat_dynamic_age',\n",
        " 'past_feat_dynamic_real',\n",
        " 'future_feat_dynamic_real',\n",
        " 'past_observed_values',\n",
        " 'future_observed_values',\n",
        " 'past_target',\n",
        " 'future_target',\n",
        " 'past_is_pad',\n",
        " 'forecast_start']\n",
        " '''\n",
        "print ('start:', test_tf_entry['start'])\n",
        "print ('feat_static_cat:', test_tf_entry['feat_static_cat'].shape)\n",
        "print ('source:', test_tf_entry['source'])\n",
        "print ('past_feat_dynamic_age:', test_tf_entry['past_feat_dynamic_age'].shape)\n",
        "print ('future_feat_dynamic_age:', test_tf_entry['future_feat_dynamic_age'].shape)\n",
        "print ('past_feat_dynamic_real:', test_tf_entry['past_feat_dynamic_real'].shape)\n",
        "print ('future_feat_dynamic_real:', test_tf_entry['future_feat_dynamic_real'].shape) ## 이게 왜 0일까??? 주는 방법은 없나?????\n",
        "print ('past_observed_values:', test_tf_entry['past_observed_values'].shape)\n",
        "print ('future_observed_values:', test_tf_entry['future_observed_values'].shape)\n",
        "print ('past_target:', test_tf_entry['past_target'].shape)\n",
        "print ('future_target:', test_tf_entry['future_target'].shape)\n",
        "print ('past_is_pad:', test_tf_entry['past_is_pad'].shape)\n",
        "print ('forecast_start:', test_tf_entry['forecast_start'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z1E6EzBy5WjF",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "print ('start:', test_tf_entry['start'])\n",
        "print ('feat_static_cat:', test_tf_entry['feat_static_cat'])\n",
        "print ('source:', test_tf_entry['source'])\n",
        "print ('past_feat_dynamic_age:', test_tf_entry['past_feat_dynamic_age'])\n",
        "print ('future_feat_dynamic_age:', test_tf_entry['future_feat_dynamic_age'])\n",
        "print ('past_feat_dynamic_real:', test_tf_entry['past_feat_dynamic_real'])\n",
        "print ('future_feat_dynamic_real:', test_tf_entry['future_feat_dynamic_real'])\n",
        "print ('past_observed_values:', test_tf_entry['past_observed_values'])\n",
        "print ('future_observed_values:', test_tf_entry['future_observed_values'])\n",
        "print ('past_target:', test_tf_entry['past_target'])\n",
        "print ('future_target:', test_tf_entry['future_target'])\n",
        "print ('past_is_pad:', test_tf_entry['past_is_pad'])\n",
        "print ('forecast_start:', test_tf_entry['forecast_start'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w3R5D0RU5WjI",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## source가 row=0이고\n",
        "## start: 2019-01-01 00:00:00,  forecast_start: 2019-01-08 00:00:00 이며, (test mode)\n",
        "## context_length가 48이므로,\n",
        "print (test_tf_entry['past_target'])\n",
        "print (target[0][-48:])\n",
        "print (len(target[0]))\n",
        "print (7*24)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lGYcNhdi5WjP",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "test_tf = transformation(iter(test_ds), is_train=False)\n",
        "test_gen = iter(test_tf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nj12jofb5WjS",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "## randomly sampled results 확인\n",
        "\n",
        "for i in range(110) :\n",
        "  test_tmp_entry = next(test_gen)\n",
        "  #print (train_tmp_entry['start'])\n",
        "  print (i, test_tmp_entry['source'], \n",
        "         test_tmp_entry['forecast_start'] , \n",
        "         (test_tmp_entry['forecast_start'] - test_tmp_entry['start'] ) \n",
        "         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nCgyerRIBcn",
        "colab_type": "text"
      },
      "source": [
        "# 3. Training an existing model\n",
        "\n",
        "GluonTS comes with a number of pre-built models. All the user needs to do is configure some hyperparameters. The existing models focus on (but are not limited to) probabilistic forecasting. Probabilistic forecasts are predictions in the form of a probability distribution, rather than simply a single point estimate. Having estimated the future distribution of each time step in the forecasting horizon, we can draw a sample from the distribution at each time step and thus create a \"sample path\" that can be seen as a possible realization of the future. In practice we draw multiple samples and create multiple sample paths which can be used for visualization, evaluation of the model, to derive statistics, etc.\n",
        "\n",
        "## 3.1 Configuring an estimator\n",
        "\n",
        "We will begin with GulonTS's pre-built feedforward neural network estimator, a simple but powerful forecasting model. We will use this model to demonstrate the process of training a model, producing forecasts, and evaluating the results.\n",
        "\n",
        "GluonTS's built-in feedforward neural network (`SimpleFeedForwardEstimator`) accepts an input window of length `context_length` and predicts the distribution of the values of the subsequent `prediction_length` values. In GluonTS parlance, the feedforward neural network model is an example of `Estimator`. In GluonTS, `Estimator` objects represent a forecasting model as well as details such as its coefficients, weights, etc.\n",
        "\n",
        "In general, each estimator (pre-built or custom) is configured by a number of hyperparameters that can be either common (but not binding) among all estimators (e.g., the `prediction_length`) or specific for the particular estimator (e.g., number of layers for a neural network or the stride in a CNN).\n",
        "\n",
        "Finally, each estimator is configured by a `Trainer`, which defines how the model will be trained i.e., the number of epochs, the learning rate, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iINZaJT5IBcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
        "from gluonts.trainer import Trainer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4NoHmvCIBcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator = SimpleFeedForwardEstimator(\n",
        "    num_hidden_dimensions=[10],\n",
        "    prediction_length=custom_ds_metadata['prediction_length'],\n",
        "    context_length=2*custom_ds_metadata['prediction_length'],\n",
        "    freq=custom_ds_metadata['freq'],\n",
        "    trainer=Trainer(ctx=\"cpu\", \n",
        "                    epochs=5, \n",
        "                    learning_rate=1e-3, \n",
        "                    hybridize=False, \n",
        "                    num_batches_per_epoch=100\n",
        "                   )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6euCgFEIBcy",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Getting a predictor\n",
        "\n",
        "After specifying our estimator with all the necessary hyperparameters we can train it using our training dataset `dataset.train` by invoking the `train` method of the estimator. The training algorithm returns a fitted model (or a `Predictor` in GluonTS parlance) that can be used to construct forecasts.\n",
        "\n",
        "We should emphasize here that a single model, as the one defined above, is trained over all the time series contained in the training dataset `train_ds`. This results in a **global** model, suitable for prediction for all the time series in `train_ds` and possibly for other unseen related time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERSwvhCeIBcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor = estimator.train(train_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZyZEM7rqhW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 오래 걸림. 나중에 확인해보자!!!!\n",
        "predictor_tf = estimator.train(train_tf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l3l5kAsIBc3",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 Saving/Loading an existing model\n",
        "\n",
        "A fitted model, i.e., a `Predictor`, can be saved and loaded back easily:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYza8gCrIBc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the trained model in tmp/\n",
        "from pathlib import Path\n",
        "predictor.serialize(Path(\"/tmp/\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ktWHWUZIBc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loads it back\n",
        "from gluonts.model.predictor import Predictor\n",
        "predictor_deserialized = Predictor.deserialize(Path(\"/tmp/\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVx76viJIBc-",
        "colab_type": "text"
      },
      "source": [
        "# 4. Evaluation\n",
        "\n",
        "## 4.1 Getting the forecasts\n",
        "\n",
        "With a predictor in hand, we can now predict the last window of the `dataset.test` and evaluate our model's performance.\n",
        "\n",
        "GluonTS comes with the `make_evaluation_predictions` function that automates the process of prediction and model evaluation. Roughly, this function performs the following steps:\n",
        "\n",
        "- Removes the final window of length `prediction_length` of the `dataset.test` that we want to predict\n",
        "- The estimator uses the remaining data to predict (in the form of sample paths) the \"future\" window that was just removed\n",
        "- The module outputs the forecast sample paths and the `dataset.test` (as python generator objects)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEbeBeKMIBc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gluonts.evaluation.backtest import make_evaluation_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjbsXN29IBdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=test_ds,  # test dataset\n",
        "    predictor=predictor,  # predictor\n",
        "    num_samples=300,  # number of sample paths we want for evaluation\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjJPtJqsIBdE",
        "colab_type": "text"
      },
      "source": [
        "First, we can convert these generators to lists to ease the subsequent computations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJuAvM9FIBdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset에 있는 데이터 개수 만큼이 만들어짐\n",
        "forecasts = list(forecast_it)\n",
        "tss = list(ts_it)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ0NGPmOrsPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ \n",
        "print (len(forecasts), len(tss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gjTDEYzIBdK",
        "colab_type": "text"
      },
      "source": [
        "We can examine the first element of these lists (that corresponds to the first time series of the dataset). Let's start with the list containing the time series, i.e., `tss`. We expect the first entry of `tss` to contain the (target of the) first time series of `test_ds`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4dgdX0BIBdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first entry of the time series list\n",
        "ts_entry = tss[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dUk5BNosJOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "print (len(ts_entry)) # 24*7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8na6BrRlIBdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first 5 values of the time series (convert from pandas to numpy)\n",
        "np.array(ts_entry[:5]).reshape(-1,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1y_lZ9SIBdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first entry of test_ds\n",
        "test_ds_entry = next(iter(test_ds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O6f1iBmIBda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first 5 values\n",
        "test_ds_entry['target'][:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOVoFvB6saJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "print (test_ds_entry['target'].shape) #24*7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fvX87xfIBdc",
        "colab_type": "text"
      },
      "source": [
        "The entries in the `forecast` list are a bit more complex. They are objects that contain all the sample paths in the form of `numpy.ndarray` with dimension `(num_samples, prediction_length)`, the start date of the forecast, the frequency of the time series, etc. We can access all these information by simply invoking the corresponding attribute of the forecast object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxvwQPhkIBdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first entry of the forecast list\n",
        "forecast_entry = forecasts[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5l81QNWswH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "print (forecast_entry)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1AUfM8_IBdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Number of sample paths: {forecast_entry.num_samples}\")\n",
        "print(f\"Dimension of samples: {forecast_entry.samples.shape}\")\n",
        "print(f\"Start date of the forecast window: {forecast_entry.start_date}\")\n",
        "print(f\"Frequency of the time series: {forecast_entry.freq}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SWXzqplIBdj",
        "colab_type": "text"
      },
      "source": [
        "We can also do calculations to summarize the sample paths, such as computing the mean or a quantile for each of the 24 time steps in the forecast window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u0rIcsiIBdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Mean of the future window:\\n {forecast_entry.mean}\")\n",
        "print(f\"0.5-quantile (median) of the future window:\\n {forecast_entry.quantile(0.5)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnDe8KxqIBdr",
        "colab_type": "text"
      },
      "source": [
        "`Forecast` objects have a `plot` method that can summarize the forecast paths as the mean, prediction intervals, etc. The prediction intervals are shaded in different colors as a \"fan chart\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2txFwEMvIBdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_prob_forecasts(ts_entry, forecast_entry):\n",
        "    plot_length = 150 \n",
        "    prediction_intervals = (50.0, 90.0)\n",
        "    legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1] \n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
        "    ts_entry[-plot_length:].plot(ax=ax)  # plot the time series\n",
        "    forecast_entry.plot(prediction_intervals=prediction_intervals, color='g')\n",
        "    plt.grid(which=\"both\")\n",
        "    plt.legend(legend, loc=\"upper left\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvYa-47SuwQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "prediction_intervals = (50.0, 90.0)\n",
        "legend1 = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1] # interval sorting\n",
        "print (legend1)\n",
        "legend2 = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::1]\n",
        "print (legend2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mThCBxS7IBdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_prob_forecasts(ts_entry, forecast_entry)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-JsSyX-wZjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "def plot_prob_forecasts2(plot_length, ts_entry, forecast_entry):\n",
        "    plot_length = plot_length \n",
        "    prediction_intervals = (50.0, 90.0)\n",
        "    legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1] \n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
        "    ts_entry[-plot_length:].plot(ax=ax)  # plot the time series\n",
        "    forecast_entry.plot(prediction_intervals=prediction_intervals, color='g')\n",
        "    plt.grid(which=\"both\")\n",
        "    plt.legend(legend, loc=\"upper left\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h78e6eLBwuQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "plot_prob_forecasts2(100, ts_entry, forecast_entry)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOo-eiIuwZUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "plot_prob_forecasts2(50, ts_entry, forecast_entry)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4DHB6dSwynU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "plot_prob_forecasts2(24, ts_entry, forecast_entry)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xthAeNRKIBdw",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 Compute metrics\n",
        "\n",
        "We can also evaluate the quality of our forecasts numerically. In GluonTS, the `Evaluator` class can compute aggregate performance metrics, as well as metrics per time series (which can be useful for analyzing performance across heterogeneous time series)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xGqPbdWIBdw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gluonts.evaluation import Evaluator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-93BOAzyvQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=test_ds,  # test dataset\n",
        "    predictor=predictor,  # predictor\n",
        "    num_samples=300,  # number of sample paths we want for evaluation\n",
        ")\n",
        "\n",
        "# dataset에 있는 데이터 개수 만큼이 만들어짐\n",
        "forecasts = list(forecast_it)\n",
        "tss = list(ts_it)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMt-9IwKIBdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
        "agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_ds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SPDRRBgIBd0",
        "colab_type": "text"
      },
      "source": [
        "Aggregate metrics aggregate both across time-steps and across time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jN28VHuIBd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sampling 결과다 보니, sample paths를 다시 만들 때마다 값이 조금씩 달라지기는 함\n",
        "## 어느 시점의 값을 결과로 사용해야 하는 걸까???????????\n",
        "print(json.dumps(agg_metrics, indent=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_QmWMCgIBd2",
        "colab_type": "text"
      },
      "source": [
        "Individual metrics are aggregated only across time-steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApDJdSxeIBd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "item_metrics.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p4kojwCIBd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "item_metrics.plot(x='MSIS', y='MASE', kind='scatter')\n",
        "plt.grid(which=\"both\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szNNSH0WIBd6",
        "colab_type": "text"
      },
      "source": [
        "# 5. Create your own model\n",
        "\n",
        "For creating our own forecast model we need to:\n",
        "\n",
        "- Define the training and prediction network\n",
        "- Define a new estimator that specifies any data processing and uses the networks\n",
        "\n",
        "The training and prediction networks can be arbitrarily complex but they should follow some basic rules:\n",
        "\n",
        "- Both should have a `hybrid_forward` method that defines what should happen when the network is called    \n",
        "- The training network's `hybrid_forward` should return a **loss** based on the prediction and the true values\n",
        "- The prediction network's `hybrid_forward` should return the predictions \n",
        "\n",
        "The estimator should also follow some rules:\n",
        "\n",
        "- It should include a `create_transformation` method that defines all the possible feature transformations and how the data is split during training\n",
        "- It should include a `create_training_network` method that returns the training network configured with any necessary hyperparameters\n",
        "- It should include a `create_predictor` method that creates the prediction network, and returns a `Predictor` object \n",
        "\n",
        "A `Predictor` defines the `predictor.predict` method of a given predictor. This method takes the test dataset, it passes it through the prediction network to take the predictions, and yields the predictions. You can think of the `Predictor` object as a wrapper of the prediction network that defines its `predict` method. \n",
        "\n",
        "In this section, we will start simple by creating a feedforward network that is restricted to point forecasts. Then, we will add complexity to the network by expanding it to probabilistic forecasting, considering features and scaling of the time series, and in the end we will replace it with an RNN.\n",
        "\n",
        "We need to emphasize that the way the following models are implemented and all the design choices that are made are neither binding nor necessarily optimal. Their sole purpose is to provide guidelines and hints on how to build a model. \n",
        "\n",
        "## 5.1 Point forecasts with a simple feedforward network\n",
        "\n",
        "We can create a simple training network that defines a neural network that takes as input a window of length `context_length` and predicts the subsequent window of dimension `prediction_length` (thus, the output dimension of the network is `prediction_length`). The `hybrid_forward` method of the training network returns the mean of the L1 loss. \n",
        "\n",
        "The prediction network is (and should be) identical to the training network (by inheriting the class) and its `hybrid_forward` method returns the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHKyMppjIBd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyNetwork(gluon.HybridBlock):\n",
        "    def __init__(self, prediction_length, num_cells, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.prediction_length = prediction_length\n",
        "        self.num_cells = num_cells\n",
        "\n",
        "        with self.name_scope():\n",
        "            # Set up a 3 layer neural network that directly predicts the target values\n",
        "            self.nn = mx.gluon.nn.HybridSequential()\n",
        "            self.nn.add(mx.gluon.nn.Dense(units=self.num_cells, activation='relu'))\n",
        "            self.nn.add(mx.gluon.nn.Dense(units=self.num_cells, activation='relu'))\n",
        "            self.nn.add(mx.gluon.nn.Dense(units=self.prediction_length, activation='softrelu'))\n",
        "\n",
        "\n",
        "class MyTrainNetwork(MyNetwork):\n",
        "    def hybrid_forward(self, F, past_target, future_target):\n",
        "        prediction = self.nn(past_target)\n",
        "        # calculate L1 loss with the future_target to learn the median\n",
        "        return (prediction - future_target).abs().mean(axis=-1)\n",
        "\n",
        "\n",
        "class MyPredNetwork(MyTrainNetwork):\n",
        "    # The prediction network only receives past_target and returns predictions\n",
        "    def hybrid_forward(self, F, past_target):\n",
        "        prediction = self.nn(past_target)\n",
        "        return prediction.expand_dims(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqA15181IBd8",
        "colab_type": "text"
      },
      "source": [
        "The estimator class is configured by a few hyperparameters and implements the required methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvL7J1YCIBd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gluonts.model.estimator import GluonEstimator\n",
        "from gluonts.model.predictor import Predictor, RepresentableBlockPredictor\n",
        "from gluonts.core.component import validated\n",
        "from gluonts.trainer import Trainer\n",
        "from gluonts.support.util import copy_parameters\n",
        "from gluonts.transform import ExpectedNumInstanceSampler, Transformation, InstanceSplitter\n",
        "from mxnet.gluon import HybridBlock"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwtc-MOHIBd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyEstimator(GluonEstimator):\n",
        "    @validated()\n",
        "    def __init__(\n",
        "        self,\n",
        "        prediction_length: int,\n",
        "        context_length: int,\n",
        "        freq: str,\n",
        "        num_cells: int,\n",
        "        trainer: Trainer = Trainer()\n",
        "    ) -> None:\n",
        "        super().__init__(trainer=trainer)\n",
        "        self.prediction_length = prediction_length\n",
        "        self.context_length = context_length\n",
        "        self.freq = freq\n",
        "        self.num_cells = num_cells\n",
        "            \n",
        "    def create_transformation(self):\n",
        "        # Feature transformation that the model uses for input\n",
        "        # Here we use a transformation that defines only how the train and test windows are cut\n",
        "        return InstanceSplitter(\n",
        "                    target_field=FieldName.TARGET,\n",
        "                    is_pad_field=FieldName.IS_PAD,\n",
        "                    start_field=FieldName.START,\n",
        "                    forecast_start_field=FieldName.FORECAST_START,\n",
        "                    train_sampler=ExpectedNumInstanceSampler(num_instances=1),\n",
        "                    past_length=self.context_length,\n",
        "                    future_length=self.prediction_length,\n",
        "                )\n",
        "    \n",
        "    def create_training_network(self) -> MyTrainNetwork:\n",
        "        return MyTrainNetwork(\n",
        "            prediction_length=self.prediction_length,\n",
        "            num_cells = self.num_cells\n",
        "        )\n",
        "\n",
        "    def create_predictor(\n",
        "        self, transformation: Transformation, trained_network: HybridBlock\n",
        "    ) -> Predictor:\n",
        "        prediction_network = MyPredNetwork(\n",
        "            prediction_length=self.prediction_length,\n",
        "            num_cells=self.num_cells\n",
        "        )\n",
        "\n",
        "        copy_parameters(trained_network, prediction_network)\n",
        "\n",
        "        return RepresentableBlockPredictor(\n",
        "            input_transform=transformation,\n",
        "            prediction_net=prediction_network,\n",
        "            batch_size=self.trainer.batch_size,\n",
        "            freq=self.freq,\n",
        "            prediction_length=self.prediction_length,\n",
        "            ctx=self.trainer.ctx,\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eH7bfPvIBeC",
        "colab_type": "text"
      },
      "source": [
        "After defining the training and prediction network, as well as the estimator class, we can follow exactly the same steps as with the existing models, i.e., we can specify our estimator by passing all the required hyperparameters to the estimator class, train the estimator by invoking its `train` method to create a predictor, and finally use the `make_evaluation_predictions` function to generate our forecasts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVoT190WIBeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator = MyEstimator(\n",
        "    prediction_length=custom_ds_metadata['prediction_length'],\n",
        "    context_length=2*custom_ds_metadata['prediction_length'],\n",
        "    freq=custom_ds_metadata['freq'],\n",
        "    num_cells=40,\n",
        "    trainer=Trainer(ctx=\"cpu\", \n",
        "                    epochs=5, \n",
        "                    learning_rate=1e-3, \n",
        "                    hybridize=False, \n",
        "                    num_batches_per_epoch=100\n",
        "                   )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu917M_iIBeF",
        "colab_type": "text"
      },
      "source": [
        "The estimator can be trained using our training dataset `train_ds` just by invoking its `train` method. The training returns a predictor that can be used to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR94rUXnIBeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor = estimator.train(train_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsDvSKmAIBeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=test_ds,  # test dataset\n",
        "    predictor=predictor,  # predictor\n",
        "    num_samples=100,  # number of sample paths we want for evaluation\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nlh_3GSsIBeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forecasts = list(forecast_it)\n",
        "tss = list(ts_it)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S-fUS_GIBeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_prob_forecasts(tss[0], forecasts[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buIcwzUq-W0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=test_ds,  # test dataset\n",
        "    predictor=predictor,  # predictor\n",
        "    num_samples=100,  # number of sample paths we want for evaluation\n",
        ")\n",
        "\n",
        "forecasts = list(forecast_it)\n",
        "tss = list(ts_it)\n",
        "\n",
        "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
        "agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_ds))\n",
        "\n",
        "print(json.dumps(agg_metrics, indent=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVr-4cMS6Khl",
        "colab_type": "text"
      },
      "source": [
        "Observe that we cannot actually see any prediction intervals in the predictions. This is expected since the model that we defined does not do probabilistic forecasting but it just gives point estimates. By requiring 100 sample paths (defined in `make_evaluation_predictions`) in such a network, we get 100 times the same output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEtmMGweIBeS",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## 5.2 Probabilistic forecasting\n",
        "\n",
        "### 5.2.1 How does a model learn a distribution?\n",
        "\n",
        "Probabilistic forecasting requires that we learn the distribution of the future values of the time series and not the values themselves as in point forecasting. To achieve this, we need to specify the type of distribution that the future values follow. GluonTS comes with a number of different distributions that cover many use cases, such as Gaussian, Student-t and Uniform just to name a few.  \n",
        "\n",
        "\n",
        "In order to learn a distribution we need to learn its parameters. For example, in the simple case where we assume a Gaussian distribution, we need to learn the mean and the variance that fully specify the distribution.\n",
        "\n",
        "Each distribution that is available in GluonTS is defined by the corresponding `Distribution` class (e.g., `Gaussian`). This class defines -among others- the parameters of the distribution, its (log-)likelihood and a sampling method (given the parameters). \n",
        "\n",
        "However, it is not straightforward how to connect a model with such a distribution and learn its parameters. For this, each distribution comes with a `DistributionOutput` class (e.g., `GaussianOutput`). The role of this class is to connect a model with a distribution. Its main usage is to take the output of the model and map it to the parameters of the distribution. You can think of it as an additional projection layer on top of the model. The parameters of this layer are optimized along with the rest of the network.\n",
        "\n",
        "By including this projection layer, our model effectively learns the parameters of the (chosen) distribution of each time step. Such a model is usually optimized by choosing as a loss function the negative log-likelihood of the chosen distribution. After we optimize our model and learn the parameters we can sample or derive any other useful statistics from the learned distributions.\n",
        "\n",
        "### 5.2.2 Feedforward network for probabilistic forecasting\n",
        "\n",
        "Let's see what changes we need to make to the previous model to make it probabilistic: \n",
        "\n",
        "- First, we need to change the output of the network. In the point forecast network the output was a vector of length `prediction_length` that gave directly the point estimates. Now, we need to output a set of features that the `DistributionOutput` will use to project to the distribution parameters. These features should be different for each time step at the prediction horizon. Therefore we need an overall output of `prediction_length * num_features` values.\n",
        "- The `DistributionOutput` takes as input a tensor and uses the last dimension as features to be projected to the distribution parameters. Here, we need a distribution object for each time step, i.e., `prediction_length` distribution objects. Given that the output of the network has `prediction_length * num_features` values, we can reshape it to `(prediction_length, num_features)` and get the required distributions, while the last axis of length `num_features` will be projected to the distribution parameters. \n",
        "- We want the prediction network to output many sample paths for each time series. To achieve this we can repeat each time series as many times as the number of sample paths and do a standard forecast for each of them. \n",
        "\n",
        "Note that in all the tensors that we handle there is an initial dimension that refers to the batch, e.g., the output of the network has dimension `(batch_size, prediction_length * num_features)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCt51zVtIBeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gluonts.distribution.distribution_output import DistributionOutput\n",
        "from gluonts.distribution.gaussian import GaussianOutput"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU2i01ePIBeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyProbNetwork(gluon.HybridBlock):\n",
        "    def __init__(self, \n",
        "                 prediction_length, \n",
        "                 distr_output, ## 추가됨\n",
        "                 num_cells, \n",
        "                 num_sample_paths=100, ## 추가됨\n",
        "                 **kwargs\n",
        "    ) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "        self.prediction_length = prediction_length\n",
        "        self.distr_output = distr_output # 추가됨\n",
        "        self.num_cells = num_cells\n",
        "        self.num_sample_paths = num_sample_paths # 추가됨\n",
        "        self.proj_distr_args = distr_output.get_args_proj() # 추가됨\n",
        "\n",
        "        with self.name_scope():\n",
        "            # Set up a 2 layer neural network that its ouput will be projected to the distribution parameters\n",
        "            self.nn = mx.gluon.nn.HybridSequential()\n",
        "            self.nn.add(mx.gluon.nn.Dense(units=self.num_cells, activation='relu'))\n",
        "            self.nn.add(mx.gluon.nn.Dense(units=self.prediction_length * self.num_cells, activation='relu')) # output 개수 달라짐\n",
        "            ## 마지막 softrelu layer가 빠짐(self.nn.add(mx.gluon.nn.Dense(units=self.prediction_length, activation='softrelu')))\n",
        "\n",
        "class MyProbTrainNetwork(MyProbNetwork):\n",
        "    def hybrid_forward(self, F, past_target, future_target):\n",
        "        # compute network output\n",
        "        net_output = self.nn(past_target)\n",
        "\n",
        "        # (batch, prediction_length * nn_features)  ->  (batch, prediction_length, nn_features)\n",
        "        net_output = net_output.reshape(0, self.prediction_length, -1) ## 추가됨\n",
        "\n",
        "        # project network output to distribution parameters domain\n",
        "        distr_args = self.proj_distr_args(net_output)  ## 추가됨\n",
        "\n",
        "        # compute distribution\n",
        "        distr = self.distr_output.distribution(distr_args)  ## 추가됨\n",
        "\n",
        "        # negative log-likelihood\n",
        "        loss = distr.loss(future_target)  ## 추가됨\n",
        "        return loss\n",
        "\n",
        "\n",
        "class MyProbPredNetwork(MyProbTrainNetwork):\n",
        "    # The prediction network only receives past_target and returns predictions\n",
        "    def hybrid_forward(self, F, past_target):\n",
        "        # repeat past target: from (batch_size, past_target_length) to \n",
        "        # (batch_size * num_sample_paths, past_target_length)\n",
        "        repeated_past_target = past_target.repeat(  # sample_paths를 한번에 만들기 위해 repeat로 넣는 듯\n",
        "            repeats=self.num_sample_paths, axis=0\n",
        "        )\n",
        "        \n",
        "        # compute network output\n",
        "        net_output = self.nn(repeated_past_target)\n",
        "\n",
        "        # (batch * num_sample_paths, prediction_length * nn_features)  ->  (batch * num_sample_paths, prediction_length, nn_features)\n",
        "        net_output = net_output.reshape(0, self.prediction_length, -1)\n",
        "       \n",
        "        # project network output to distribution parameters domain\n",
        "        distr_args = self.proj_distr_args(net_output)\n",
        "\n",
        "        # compute distribution\n",
        "        distr = self.distr_output.distribution(distr_args)\n",
        "\n",
        "        # get (batch_size * num_sample_paths, prediction_length) samples\n",
        "        samples = distr.sample()\n",
        "        \n",
        "        # reshape from (batch_size * num_sample_paths, prediction_length) to \n",
        "        # (batch_size, num_sample_paths, prediction_length)\n",
        "        return samples.reshape(shape=(-1, self.num_sample_paths, self.prediction_length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN9kYaK6IBeX",
        "colab_type": "text"
      },
      "source": [
        "The changes we need to do at the estimator are minor and they mainly reflect the additional `distr_output` parameter that our networks use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9ZeX0p6IBeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyProbEstimator(GluonEstimator):\n",
        "    @validated()\n",
        "    def __init__(\n",
        "            self,\n",
        "            prediction_length: int,\n",
        "            context_length: int,\n",
        "            freq: str,\n",
        "            distr_output: DistributionOutput, # 추가됨\n",
        "            num_cells: int,\n",
        "            num_sample_paths: int = 100, # 추가됨\n",
        "            trainer: Trainer = Trainer()\n",
        "    ) -> None:\n",
        "        super().__init__(trainer=trainer)\n",
        "        self.prediction_length = prediction_length\n",
        "        self.context_length = context_length\n",
        "        self.freq = freq\n",
        "        self.distr_output = distr_output # 추가됨\n",
        "        self.num_cells = num_cells\n",
        "        self.num_sample_paths = num_sample_paths # 추가됨\n",
        "\n",
        "    def create_transformation(self):\n",
        "        return InstanceSplitter(\n",
        "            target_field=FieldName.TARGET,\n",
        "            is_pad_field=FieldName.IS_PAD,\n",
        "            start_field=FieldName.START,\n",
        "            forecast_start_field=FieldName.FORECAST_START,\n",
        "            train_sampler=ExpectedNumInstanceSampler(num_instances=1),\n",
        "            past_length=self.context_length,\n",
        "            future_length=self.prediction_length,\n",
        "        )\n",
        "\n",
        "    def create_training_network(self) -> MyProbTrainNetwork:\n",
        "        return MyProbTrainNetwork(\n",
        "            prediction_length=self.prediction_length,\n",
        "            distr_output=self.distr_output, # 추가됨\n",
        "            num_cells=self.num_cells,\n",
        "            num_sample_paths=self.num_sample_paths # 추가됨\n",
        "        )\n",
        "\n",
        "    def create_predictor(\n",
        "            self, transformation: Transformation, trained_network: HybridBlock\n",
        "    ) -> Predictor:\n",
        "        prediction_network = MyProbPredNetwork(\n",
        "            prediction_length=self.prediction_length,\n",
        "            distr_output=self.distr_output, # 추가됨\n",
        "            num_cells=self.num_cells,\n",
        "            num_sample_paths=self.num_sample_paths # 추가됨\n",
        "        )\n",
        "\n",
        "        copy_parameters(trained_network, prediction_network)\n",
        "\n",
        "        return RepresentableBlockPredictor(\n",
        "            input_transform=transformation,\n",
        "            prediction_net=prediction_network,\n",
        "            batch_size=self.trainer.batch_size,\n",
        "            freq=self.freq,\n",
        "            prediction_length=self.prediction_length,\n",
        "            ctx=self.trainer.ctx,\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S4lOy1uIBed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator = MyProbEstimator(\n",
        "    prediction_length=custom_ds_metadata['prediction_length'],\n",
        "    context_length=2*custom_ds_metadata['prediction_length'],\n",
        "    freq=custom_ds_metadata['freq'],\n",
        "    distr_output=GaussianOutput(), # 추가됨\n",
        "    num_cells=40,\n",
        "    trainer=Trainer(ctx=\"cpu\", \n",
        "                    epochs=5, \n",
        "                    learning_rate=1e-3, \n",
        "                    hybridize=False, \n",
        "                    num_batches_per_epoch=100\n",
        "                   )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nky39IMEIBef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor = estimator.train(train_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpUNlKxIIBeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=test_ds,  # test dataset\n",
        "    predictor=predictor,  # predictor\n",
        "    num_samples=100,  # number of sample paths we want for evaluation\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC91XEgfIBek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forecasts = list(forecast_it)\n",
        "tss = list(ts_it)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdvThjpNIBen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_prob_forecasts(tss[0], forecasts[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "homoXYBn9FVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=test_ds,  # test dataset\n",
        "    predictor=predictor,  # predictor\n",
        "    num_samples=100,  # number of sample paths we want for evaluation\n",
        ")\n",
        "\n",
        "forecasts = list(forecast_it)\n",
        "tss = list(ts_it)\n",
        "\n",
        "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
        "agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_ds))\n",
        "\n",
        "print(json.dumps(agg_metrics, indent=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wYwrya_IBep",
        "colab_type": "text"
      },
      "source": [
        "## 5.3 Add features and scaling\n",
        "\n",
        "In the previous networks we used only the target and did not leverage any of the features of the dataset. Here we expand the probabilistic network by including the `feat_dynamic_real` field of the dataset that could enhance the forecasting power of our model. We achieve this by concatenating the target and the features to an enhanced vector that forms the new network input. \n",
        "\n",
        "All the features that are available in a dataset can be potentially used as inputs to our model. However, for the purposes of this example we will restrict ourselves to using only one feature.\n",
        "\n",
        "An important issue that a practitioner needs to deal with often is the different orders of magnitude in the values of the time series in a dataset. It is extremely helpful for a model to be trained and forecast values that lie roughly in the same value range. To address this issue, we add a `Scaler` to out model, that computes the scale of each time series. Then we can scale accordingly the values of the time series or any related features and use these as inputs to the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBpK0ovVIBep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gluonts.block.scaler import MeanScaler, NOPScaler\n",
        "\n",
        "\"\"\"\n",
        "The ``MeanScaler`` computes a per-item scale according to the average\n",
        "    absolute value over time of each item. The average is computed only among\n",
        "    the observed values in the data tensor, as indicated by the second\n",
        "    argument. Items with no observed data are assigned a scale based on the\n",
        "    global average.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "The ``NOPScaler`` assigns a scale equals to 1 to each input item, i.e.,\n",
        "    no scaling is applied upon calling the ``NOPScaler``.\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RInLyU8mIBer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyProbNetwork(gluon.HybridBlock):\n",
        "    def __init__(self, \n",
        "                 prediction_length, \n",
        "                 context_length, # 추가됨\n",
        "                 distr_output, \n",
        "                 num_cells, \n",
        "                 num_sample_paths=100, \n",
        "                 scaling=True, # 추가됨\n",
        "                 **kwargs\n",
        "    ) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "        self.prediction_length = prediction_length\n",
        "        self.context_length = context_length # 추가됨\n",
        "        self.distr_output = distr_output\n",
        "        self.num_cells = num_cells\n",
        "        self.num_sample_paths = num_sample_paths\n",
        "        self.proj_distr_args = distr_output.get_args_proj()\n",
        "        self.scaling = scaling # 추가됨\n",
        "\n",
        "        with self.name_scope():\n",
        "            # Set up a 2 layer neural network that its ouput will be projected to the distribution parameters\n",
        "            self.nn = mx.gluon.nn.HybridSequential()\n",
        "            self.nn.add(mx.gluon.nn.Dense(units=self.num_cells, activation='relu'))\n",
        "            self.nn.add(mx.gluon.nn.Dense(units=self.prediction_length * self.num_cells, activation='relu'))\n",
        "\n",
        "            if scaling: # 추가됨\n",
        "                self.scaler = MeanScaler(keepdims=True) # [N,C]차원의 scale 반환 ## batch(item)당[N], feature당[C] scale \n",
        "            else:\n",
        "                self.scaler = NOPScaler(keepdims=True)\n",
        "\n",
        "    def compute_scale(self, past_target, past_observed_values): # 추가됨(data, observed_indicator가 입력되는 듯)\n",
        "        # scale shape is (batch_size, 1)\n",
        "        _, scale = self.scaler(\n",
        "            past_target.slice_axis(\n",
        "                axis=1, begin=-self.context_length, end=None\n",
        "            ),\n",
        "            past_observed_values.slice_axis(\n",
        "                axis=1, begin=-self.context_length, end=None\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        return scale\n",
        "\n",
        "\n",
        "class MyProbTrainNetwork(MyProbNetwork):\n",
        "    def hybrid_forward(self, F, past_target, future_target, past_observed_values, past_feat_dynamic_real): # 늘어남\n",
        "        # compute scale \n",
        "        scale = self.compute_scale(past_target, past_observed_values) # 추가됨\n",
        "\n",
        "        # scale target and time features\n",
        "        past_target_scale = F.broadcast_div(past_target, scale) # 추가됨\n",
        "        past_feat_dynamic_real_scale = F.broadcast_div(past_feat_dynamic_real.squeeze(axis=-1), scale) # 추가됨\n",
        "\n",
        "        # concatenate target and time features to use them as input to the network\n",
        "        net_input = F.concat(past_target_scale, past_feat_dynamic_real_scale, dim=-1) # 추가됨\n",
        "\n",
        "        # compute network output\n",
        "        net_output = self.nn(net_input) # 변경됨\n",
        "\n",
        "        # (batch, prediction_length * nn_features)  ->  (batch, prediction_length, nn_features)\n",
        "        net_output = net_output.reshape(0, self.prediction_length, -1)\n",
        "\n",
        "        # project network output to distribution parameters domain\n",
        "        distr_args = self.proj_distr_args(net_output)\n",
        "\n",
        "        # compute distribution\n",
        "        distr = self.distr_output.distribution(distr_args, scale=scale) # 늘어남\n",
        "\n",
        "        # negative log-likelihood\n",
        "        loss = distr.loss(future_target)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class MyProbPredNetwork(MyProbTrainNetwork):\n",
        "    # The prediction network only receives past_target and returns predictions('future_target'이 없어짐)\n",
        "    def hybrid_forward(self, F, past_target, past_observed_values, past_feat_dynamic_real): # 늘어남\n",
        "        # repeat fields: from (batch_size, past_target_length) to\n",
        "        # (batch_size * num_sample_paths, past_target_length) # 동시에 num_sample_paths만큼 뽑겠다.\n",
        "        repeated_past_target = past_target.repeat(\n",
        "            repeats=self.num_sample_paths, axis=0\n",
        "        )\n",
        "        repeated_past_observed_values = past_observed_values.repeat( # 추가됨\n",
        "            repeats=self.num_sample_paths, axis=0\n",
        "        )\n",
        "        repeated_past_feat_dynamic_real = past_feat_dynamic_real.repeat( # 추가됨\n",
        "            repeats=self.num_sample_paths, axis=0\n",
        "        )\n",
        "        \n",
        "        # compute scale\n",
        "        scale = self.compute_scale(repeated_past_target, repeated_past_observed_values) # 추가됨-scale 계산\n",
        "\n",
        "        # scale repeated target and time features\n",
        "        repeated_past_target_scale = F.broadcast_div(repeated_past_target, scale) # 추가됨-scale 적용\n",
        "        repeated_past_feat_dynamic_real_scale = F.broadcast_div(repeated_past_feat_dynamic_real.squeeze(axis=-1), scale) # 추가됨-scale적용\n",
        "\n",
        "        # concatenate target and time features to use them as input to the network\n",
        "        net_input = F.concat(repeated_past_target_scale, repeated_past_feat_dynamic_real_scale, dim=-1) # 추가됨\n",
        "\n",
        "        # compute network oputput\n",
        "        net_output = self.nn(net_input) # 변경됨\n",
        "        \n",
        "        # (batch * num_sample_paths, prediction_length * nn_features)  ->  (batch * num_sample_paths, prediction_length, nn_features)\n",
        "        net_output = net_output.reshape(0, self.prediction_length, -1)\n",
        "\n",
        "        # project network output to distribution parameters domain\n",
        "        distr_args = self.proj_distr_args(net_output)\n",
        "        \n",
        "        # compute distribution\n",
        "        distr = self.distr_output.distribution(distr_args, scale=scale) # 늘어남\n",
        "\n",
        "        # get (batch_size * num_sample_paths, prediction_length) samples\n",
        "        samples = distr.sample()\n",
        "\n",
        "        # reshape from (batch_size * num_sample_paths, prediction_length) to\n",
        "        # (batch_size, num_sample_paths, prediction_length)\n",
        "        return samples.reshape(shape=(-1, self.num_sample_paths, self.prediction_length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fJUyvDuIBet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyProbEstimator(GluonEstimator):\n",
        "    @validated()\n",
        "    def __init__(\n",
        "            self,\n",
        "            prediction_length: int,\n",
        "            context_length: int,\n",
        "            freq: str,\n",
        "            distr_output: DistributionOutput,\n",
        "            num_cells: int,\n",
        "            num_sample_paths: int = 100,\n",
        "            scaling: bool = True, # 추가됨\n",
        "            trainer: Trainer = Trainer()\n",
        "    ) -> None:\n",
        "        super().__init__(trainer=trainer)\n",
        "        self.prediction_length = prediction_length\n",
        "        self.context_length = context_length\n",
        "        self.freq = freq\n",
        "        self.distr_output = distr_output\n",
        "        self.num_cells = num_cells\n",
        "        self.num_sample_paths = num_sample_paths\n",
        "        self.scaling = scaling # 추가됨\n",
        "\n",
        "    def create_transformation(self):\n",
        "        # Feature transformation that the model uses for input.\n",
        "        return Chain( # 추가됨/늘어남\n",
        "            [\n",
        "                AddObservedValuesIndicator(\n",
        "                    target_field=FieldName.TARGET,\n",
        "                    output_field=FieldName.OBSERVED_VALUES,\n",
        "                ),\n",
        "                InstanceSplitter(\n",
        "                    target_field=FieldName.TARGET,\n",
        "                    is_pad_field=FieldName.IS_PAD,\n",
        "                    start_field=FieldName.START,\n",
        "                    forecast_start_field=FieldName.FORECAST_START,\n",
        "                    train_sampler=ExpectedNumInstanceSampler(num_instances=1),\n",
        "                    past_length=self.context_length,\n",
        "                    future_length=self.prediction_length,\n",
        "                    time_series_fields=[\n",
        "                        FieldName.FEAT_DYNAMIC_REAL,\n",
        "                        FieldName.OBSERVED_VALUES,\n",
        "                    ],\n",
        "                ),\n",
        "\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def create_training_network(self) -> MyProbTrainNetwork:\n",
        "        return MyProbTrainNetwork(\n",
        "            prediction_length=self.prediction_length,\n",
        "            context_length=self.context_length, # 추가됨\n",
        "            distr_output=self.distr_output,\n",
        "            num_cells=self.num_cells,\n",
        "            num_sample_paths=self.num_sample_paths,\n",
        "            scaling=self.scaling # 추가됨\n",
        "        )\n",
        "\n",
        "    def create_predictor(\n",
        "            self, transformation: Transformation, trained_network: HybridBlock\n",
        "    ) -> Predictor:\n",
        "        prediction_network = MyProbPredNetwork(\n",
        "            prediction_length=self.prediction_length,\n",
        "            context_length=self.context_length, # 추가됨\n",
        "            distr_output=self.distr_output,\n",
        "            num_cells=self.num_cells,\n",
        "            num_sample_paths=self.num_sample_paths,\n",
        "            scaling=self.scaling # 추가됨\n",
        "        )\n",
        "\n",
        "        copy_parameters(trained_network, prediction_network)\n",
        "\n",
        "        return RepresentableBlockPredictor(\n",
        "            input_transform=transformation,\n",
        "            prediction_net=prediction_network,\n",
        "            batch_size=self.trainer.batch_size,\n",
        "            freq=self.freq,\n",
        "            prediction_length=self.prediction_length,\n",
        "            ctx=self.trainer.ctx,\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCGRcf7UIBev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator = MyProbEstimator(\n",
        "    prediction_length=custom_ds_metadata['prediction_length'],\n",
        "    context_length=2*custom_ds_metadata['prediction_length'],\n",
        "    freq=custom_ds_metadata['freq'],\n",
        "    distr_output=GaussianOutput(),\n",
        "    num_cells=40,\n",
        "    trainer=Trainer(ctx=\"cpu\", \n",
        "                    epochs=5, \n",
        "                    learning_rate=1e-3, \n",
        "                    hybridize=False, \n",
        "                    num_batches_per_epoch=100\n",
        "                   )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfOunlNSUOpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next(iter(train_ds)).keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK25sxVrIBew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor = estimator.train(train_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhq_H2vbIBez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=test_ds,  # test dataset\n",
        "    predictor=predictor,  # predictor\n",
        "    num_samples=100,  # number of sample paths we want for evaluation\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apBe7COyIBe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forecasts = list(forecast_it)\n",
        "tss = list(ts_it)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iyh6iJZIBe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_prob_forecasts(tss[0], forecasts[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5R91ydO8n6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=test_ds,  # test dataset\n",
        "    predictor=predictor,  # predictor\n",
        "    num_samples=100,  # number of sample paths we want for evaluation\n",
        ")\n",
        "\n",
        "forecasts = list(forecast_it)\n",
        "tss = list(ts_it)\n",
        "\n",
        "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
        "agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_ds))\n",
        "\n",
        "print(json.dumps(agg_metrics, indent=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycb7Pje7IBe4",
        "colab_type": "text"
      },
      "source": [
        "## 5.4 From feedforward to RNN\n",
        "\n",
        "In all the previous examples we have used a feedforward neural network as the base for our forecasting model. The main idea behind it was to use as an input to the network a window of the time series (of length `context_length`) and train the network to forecast the following window (of length `prediction_length`). \n",
        "\n",
        "In this section we will replace the feedforward network with a recurrent neural network (RNN). Due to the different nature of RNNs the structure of the networks will be a bit different. Let's see what are the major changes. \n",
        "\n",
        "### 5.4.1 Training\n",
        "\n",
        "The main idea behind RNN is the same as in the feedforward networks we already constructed: as we unrolll the RNN at each time step we use as an input past values of the time series and forecast the next value. We can enhance the input by using multiple past values (for example specific lags based on seasonality patterns) or available features. However, in this example we will keep things simple and just use the last value of the time series. The output of the network at each time step is the distribution of the value of the next time step, where the state of the RNN is used as the feature vector for the parameter projection of the distribution.\n",
        "\n",
        "Due to the sequential nature of the RNN, the distinction between `past_` and `future_` in the cut window of the time series is not really necessary. Therefore, we can concatenate `past_target` and `future_target ` and treat it as a concrete `target` window that we wish to forecast. This means that the input to the RNN would be (sequentially) the window `target[-(context_length + prediction_length + 1):-1]` (one time step before the window we want to predict). As a consequence, we need to have `context_length + prediction_length + 1` available values at each window that we cut. We can define this in the `InstanceSplitter`. \n",
        "\n",
        "Overall, during training the steps are the following:\n",
        "\n",
        "- We pass sequentially through the RNN the target values `target[-(context_length + prediction_length + 1):-1]` \n",
        "- We use the state of the RNN at each time step as a feature vector and project it to the distribution parameter domain\n",
        "- The output at each time step is the distribution of the values of the next time step, which overall is the forecasted distribution for the window `target[-(context_length + prediction_length):]`\n",
        "\n",
        "The above steps are implemented in the `unroll_encoder` method.\n",
        "\n",
        "### 5.4.2 Inference\n",
        "\n",
        "During inference we know the values only of `past_target` therefore we cannot follow exactly the same steps as in training. However the main idea is very similar:\n",
        "\n",
        "- We pass sequentially through the RNN the past target values `past_target[-(context_length + 1):]` that effectively updates the state of the RNN\n",
        "- In the last time step the output of the RNN is effectively the distribution of the next value of the time series (which we do not know). Therefore we sample (`num_sample_paths` times) from this distribution and use the samples as inputs to the RNN for the next time step\n",
        "- We repeat the previous step `prediction_length` times \n",
        "\n",
        "The first step is implemented in `unroll_encoder` and the last steps in the `sample_decoder` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-6hKVTKIBe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyProbRNN(gluon.HybridBlock):\n",
        "    def __init__(self,\n",
        "                 prediction_length,\n",
        "                 context_length,\n",
        "                 distr_output,\n",
        "                 num_cells,\n",
        "                 num_layers, # 추가됨\n",
        "                 num_sample_paths=100,\n",
        "                 scaling=True,\n",
        "                 **kwargs\n",
        "     ) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "        self.prediction_length = prediction_length\n",
        "        self.context_length = context_length\n",
        "        self.distr_output = distr_output\n",
        "        self.num_cells = num_cells\n",
        "        self.num_layers = num_layers # 추가됨\n",
        "        self.num_sample_paths = num_sample_paths\n",
        "        self.proj_distr_args = distr_output.get_args_proj()\n",
        "        self.scaling = scaling\n",
        "\n",
        "        with self.name_scope():\n",
        "            self.rnn = mx.gluon.rnn.HybridSequentialRNNCell() # 변경됨\n",
        "            for k in range(self.num_layers): # 추가됨-LSTM 구조를 residualcell로 정의함\n",
        "                cell = mx.gluon.rnn.LSTMCell(hidden_size=self.num_cells)\n",
        "                cell = mx.gluon.rnn.ResidualCell(cell) if k > 0 else cell\n",
        "                self.rnn.add(cell)\n",
        "\n",
        "            if scaling:\n",
        "                self.scaler = MeanScaler(keepdims=True)\n",
        "            else:\n",
        "                self.scaler = NOPScaler(keepdims=True)\n",
        "\n",
        "    def compute_scale(self, past_target, past_observed_values):\n",
        "        # scale is computed on the context length last units of the past target\n",
        "        # scale shape is (batch_size, 1, *target_shape)\n",
        "        _, scale = self.scaler(\n",
        "            past_target.slice_axis(\n",
        "                axis=1, begin=-self.context_length, end=None\n",
        "            ),\n",
        "            past_observed_values.slice_axis(\n",
        "                axis=1, begin=-self.context_length, end=None\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        return scale\n",
        "    # 본 예제에서는 Encoder/Decoder를 같은 구조라 가정하고 정의하고 있음\n",
        "    def unroll_encoder(self, # 추가됨\n",
        "                       F, \n",
        "                       past_target, \n",
        "                       past_observed_values, \n",
        "                       future_target=None, \n",
        "                       future_observed_values=None):\n",
        "        # overall target field\n",
        "        # input target from -(context_length + prediction_length + 1) to -1\n",
        "        if future_target is not None:  # during training\n",
        "            target_in = F.concat(\n",
        "                past_target, future_target, dim=-1\n",
        "            ).slice_axis(\n",
        "                axis=1, begin=-(self.context_length + self.prediction_length + 1), end=-1\n",
        "            )\n",
        "\n",
        "            # overall observed_values field\n",
        "            # input observed_values corresponding to target_in\n",
        "            observed_values_in = F.concat(\n",
        "                past_observed_values, future_observed_values, dim=-1\n",
        "            ).slice_axis(\n",
        "                axis=1, begin=-(self.context_length + self.prediction_length + 1), end=-1\n",
        "            )\n",
        "\n",
        "            rnn_length = self.context_length + self.prediction_length\n",
        "        else:  # during inference\n",
        "            target_in = past_target.slice_axis(\n",
        "                axis=1, begin=-(self.context_length + 1), end=-1\n",
        "            )\n",
        "\n",
        "            # overall observed_values field\n",
        "            # input observed_values corresponding to target_in\n",
        "            observed_values_in = past_observed_values.slice_axis(\n",
        "                axis=1, begin=-(self.context_length + 1), end=-1\n",
        "            )\n",
        "\n",
        "            rnn_length = self.context_length\n",
        "\n",
        "        # compute scale\n",
        "        scale = self.compute_scale(target_in, observed_values_in)\n",
        "\n",
        "        # scale target_in\n",
        "        target_in_scale = F.broadcast_div(target_in, scale)\n",
        "\n",
        "        # compute network output\n",
        "        net_output, states = self.rnn.unroll(\n",
        "            inputs=target_in_scale,\n",
        "            length=rnn_length,\n",
        "            layout=\"NTC\",\n",
        "            merge_outputs=True,\n",
        "        )\n",
        "\n",
        "        return net_output, states, scale\n",
        "\n",
        "\n",
        "class MyProbTrainRNN(MyProbRNN):\n",
        "    def hybrid_forward(self,\n",
        "                       F,\n",
        "                       past_target,\n",
        "                       future_target,\n",
        "                       past_observed_values,\n",
        "                       future_observed_values): # 추가됨\n",
        "\n",
        "        net_output, _, scale = self.unroll_encoder(F, # 추가됨\n",
        "                                                   past_target,\n",
        "                                                   past_observed_values,\n",
        "                                                   future_target,\n",
        "                                                   future_observed_values)\n",
        "        # loss 연산은 encoder 부분에서도 발생함\n",
        "        # output target from -(context_length + prediction_length) to end # 추가됨\n",
        "        target_out = F.concat(\n",
        "            past_target, future_target, dim=-1\n",
        "        ).slice_axis(\n",
        "            axis=1, begin=-(self.context_length + self.prediction_length), end=None\n",
        "        )\n",
        "\n",
        "        # project network output to distribution parameters domain\n",
        "        distr_args = self.proj_distr_args(net_output)\n",
        "\n",
        "        # compute distribution\n",
        "        distr = self.distr_output.distribution(distr_args, scale=scale)\n",
        "\n",
        "        # negative log-likelihood\n",
        "        loss = distr.loss(target_out)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class MyProbPredRNN(MyProbTrainRNN):\n",
        "    # decoder부분을 predcition에서 별도로 정의(train때와 다르게 예측된 값을 인풋으로 넣어야 하므로)\n",
        "    def sample_decoder(self, F, past_target, states, scale):# 추가됨\n",
        "        # repeat fields: from (batch_size, past_target_length) to\n",
        "        # (batch_size * num_sample_paths, past_target_length)\n",
        "        repeated_states = [\n",
        "            s.repeat(repeats=self.num_sample_paths, axis=0)\n",
        "            for s in states\n",
        "        ]\n",
        "        repeated_scale = scale.repeat(repeats=self.num_sample_paths, axis=0)\n",
        "\n",
        "        # first decoder input is the last value of the past_target, i.e.,\n",
        "        # the previous value of the first time step we want to forecast\n",
        "        decoder_input = past_target.slice_axis(\n",
        "            axis=1, begin=-1, end=None\n",
        "        ).repeat(\n",
        "            repeats=self.num_sample_paths, axis=0\n",
        "        )\n",
        "\n",
        "        # list with samples at each time step\n",
        "        future_samples = []\n",
        "\n",
        "        # for each future time step we draw new samples for this time step and update the state\n",
        "        # the drawn samples are the inputs to the rnn at the next time step\n",
        "        for k in range(self.prediction_length):\n",
        "            rnn_outputs, repeated_states = self.rnn.unroll(\n",
        "                inputs=decoder_input,\n",
        "                length=1,\n",
        "                begin_state=repeated_states,\n",
        "                layout=\"NTC\",\n",
        "                merge_outputs=True,\n",
        "            )\n",
        "\n",
        "            # project network output to distribution parameters domain\n",
        "            distr_args = self.proj_distr_args(rnn_outputs)\n",
        "\n",
        "            # compute distribution\n",
        "            distr = self.distr_output.distribution(distr_args, scale=repeated_scale)\n",
        "\n",
        "            # draw samples (batch_size * num_samples, 1)\n",
        "            new_samples = distr.sample()\n",
        "\n",
        "            # append the samples of the current time step\n",
        "            future_samples.append(new_samples)\n",
        "\n",
        "            # update decoder input for the next time step\n",
        "            decoder_input = new_samples\n",
        "\n",
        "        samples = F.concat(*future_samples, dim=1)\n",
        "\n",
        "        # (batch_size, num_samples, prediction_length)\n",
        "        return samples.reshape(shape=(-1, self.num_sample_paths, self.prediction_length))\n",
        "\n",
        "    def hybrid_forward(self, F, past_target, past_observed_values): # 변경됨\n",
        "        # unroll encoder over context_length\n",
        "        net_output, states, scale = self.unroll_encoder(F,\n",
        "                                                        past_target,\n",
        "                                                        past_observed_values)\n",
        "\n",
        "        samples = self.sample_decoder(F, past_target, states, scale)\n",
        "\n",
        "        return samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBxuNWEsIBe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyProbRNNEstimator(GluonEstimator):\n",
        "    @validated()\n",
        "    def __init__(\n",
        "            self,\n",
        "            prediction_length: int,\n",
        "            context_length: int,\n",
        "            freq: str,\n",
        "            distr_output: DistributionOutput,\n",
        "            num_cells: int,\n",
        "            num_layers: int, # 추가됨\n",
        "            num_sample_paths: int = 100,\n",
        "            scaling: bool = True,\n",
        "            trainer: Trainer = Trainer()\n",
        "    ) -> None:\n",
        "        super().__init__(trainer=trainer)\n",
        "        self.prediction_length = prediction_length\n",
        "        self.context_length = context_length\n",
        "        self.freq = freq\n",
        "        self.distr_output = distr_output\n",
        "        self.num_cells = num_cells\n",
        "        self.num_layers = num_layers # 추가됨\n",
        "        self.num_sample_paths = num_sample_paths\n",
        "        self.scaling = scaling\n",
        "\n",
        "    def create_transformation(self):\n",
        "        # Feature transformation that the model uses for input.\n",
        "        return Chain(\n",
        "            [\n",
        "                AddObservedValuesIndicator(\n",
        "                    target_field=FieldName.TARGET,\n",
        "                    output_field=FieldName.OBSERVED_VALUES,\n",
        "                ),\n",
        "                InstanceSplitter(\n",
        "                    target_field=FieldName.TARGET,\n",
        "                    is_pad_field=FieldName.IS_PAD,\n",
        "                    start_field=FieldName.START,\n",
        "                    forecast_start_field=FieldName.FORECAST_START,\n",
        "                    train_sampler=ExpectedNumInstanceSampler(num_instances=1),\n",
        "                    past_length=self.context_length + 1, # 변경됨\n",
        "                    future_length=self.prediction_length,\n",
        "                    time_series_fields=[\n",
        "                        FieldName.FEAT_DYNAMIC_REAL,\n",
        "                        FieldName.OBSERVED_VALUES,\n",
        "                    ],\n",
        "                ),\n",
        "\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def create_training_network(self) -> MyProbTrainRNN:\n",
        "        return MyProbTrainRNN(\n",
        "            prediction_length=self.prediction_length,\n",
        "            context_length=self.context_length,\n",
        "            distr_output=self.distr_output,\n",
        "            num_cells=self.num_cells,\n",
        "            num_layers=self.num_layers, # 추가됨\n",
        "            num_sample_paths=self.num_sample_paths,\n",
        "            scaling=self.scaling\n",
        "        )\n",
        "\n",
        "    def create_predictor(\n",
        "            self, transformation: Transformation, trained_network: HybridBlock\n",
        "    ) -> Predictor:\n",
        "        prediction_network = MyProbPredRNN(\n",
        "            prediction_length=self.prediction_length,\n",
        "            context_length=self.context_length,\n",
        "            distr_output=self.distr_output,\n",
        "            num_cells=self.num_cells,\n",
        "            num_layers=self.num_layers, # 추가됨\n",
        "            num_sample_paths=self.num_sample_paths,\n",
        "            scaling=self.scaling\n",
        "        )\n",
        "\n",
        "        copy_parameters(trained_network, prediction_network)\n",
        "\n",
        "        return RepresentableBlockPredictor(\n",
        "            input_transform=transformation,\n",
        "            prediction_net=prediction_network,\n",
        "            batch_size=self.trainer.batch_size,\n",
        "            freq=self.freq,\n",
        "            prediction_length=self.prediction_length,\n",
        "            ctx=self.trainer.ctx,\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR80PCqFIBe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator = MyProbRNNEstimator(\n",
        "        prediction_length=24,\n",
        "        context_length=48,\n",
        "        freq=\"1H\",\n",
        "        num_cells=40,\n",
        "        num_layers=2,\n",
        "        distr_output=GaussianOutput(),\n",
        "        trainer=Trainer(ctx=\"cpu\",\n",
        "                        epochs=5,\n",
        "                        learning_rate=1e-3,\n",
        "                        hybridize=False,\n",
        "                        batch_size=32*100, # default:32\n",
        "                        num_batches_per_epoch=100/25\n",
        "                       )\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5-RvUS9IBe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor = estimator.train(train_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVQrJFtXD7F4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "print(\"Number GPU's: \" + str(mx.context.num_gpus()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TORDzYYtEzEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mxnet import gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvQezlydEEux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "# Batch_size가 엄청 클때 or 모델이 엄청 복잡할 때 GPU가 유리 \n",
        "gpu_estimator = MyProbRNNEstimator(\n",
        "        prediction_length=24,\n",
        "        context_length=48,\n",
        "        freq=\"1H\",\n",
        "        num_cells=40,\n",
        "        num_layers=2,\n",
        "        distr_output=GaussianOutput(),\n",
        "        trainer=Trainer(ctx=\"gpu\",\n",
        "                        epochs=5,\n",
        "                        learning_rate=1e-2,\n",
        "                        hybridize=False,\n",
        "                        batch_size=32*100, # default:32\n",
        "                        num_batches_per_epoch=100/25\n",
        "                       )\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5ww3RqgEEhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "gpu_predictor = gpu_estimator.train(train_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd5t-jQsIBfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=test_ds,  # test dataset\n",
        "    predictor=predictor,  # predictor\n",
        "    num_samples=100,  # number of sample paths we want for evaluation\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KATikzikIBfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forecasts = list(forecast_it)\n",
        "tss = list(ts_it)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbjGprklIBfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_prob_forecasts(tss[0], forecasts[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRkgffDZRtBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=test_ds,  # test dataset\n",
        "    predictor=predictor,  # predictor\n",
        "    num_samples=100,  # number of sample paths we want for evaluation\n",
        ")\n",
        "\n",
        "forecasts = list(forecast_it)\n",
        "tss = list(ts_it)\n",
        "\n",
        "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
        "agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_ds))\n",
        "\n",
        "print(json.dumps(agg_metrics, indent=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONvNPIr9A7QF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YJ\n",
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=test_ds,  # test dataset\n",
        "    predictor=gpu_predictor,  # predictor\n",
        "    num_samples=100,  # number of sample paths we want for evaluation\n",
        ")\n",
        "\n",
        "forecasts = list(forecast_it)\n",
        "tss = list(ts_it)\n",
        "\n",
        "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
        "agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_ds))\n",
        "\n",
        "print(json.dumps(agg_metrics, indent=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJFQ8r0eH8nC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}